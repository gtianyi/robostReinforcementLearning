@article{Lai1985,
abstract = {The authors consider multiarmed bandit problems with switching cost, define uniformly good allocation rules, and restrict attention to such rules. They present a lower bound on the asymptotic performance of uniformly good allocation rules and construct an allocation scheme that achieves the bound. It is found that despite the inclusion of a switching cost the proposed allocation scheme achieves the same asymptotic performance as the optimal rule for the bandit problem without switching cost. This is made possible by grouping together samples into blocks of increasing sizes, thereby reducing the number of switches to O(log {\textless}e1{\textgreater}n{\textless}/e1{\textgreater}). Finally, an optimal allocation scheme for a large class of distributions which includes members of the exponential family is illustrated},
author = {Lai, T. L. and Robbins, Herbert},
doi = {10.1016/0196-8858(85)90002-8},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/Lai{\_}robbins85.pdf:pdf},
isbn = {0196-8858},
issn = {10902074},
journal = {Advances in Applied Mathematics},
number = {1},
pages = {4--22},
title = {{Asymptotically efficient adaptive allocation rules}},
volume = {6},
year = {1985}
}
@article{Brafman2001,
abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algo-rithms: (1) It is simpler and more general than Kearns and Singh's E 3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the " optimism under uncertainty " bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
doi = {10.1162/153244303765208377},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/brafman02a.pdf:pdf},
isbn = {1532-4435},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {decision processes,learning in games,markov,provably efficient learning,reinforcement learning,stochastic games},
pages = {953--958},
title = {{R-MAX - A general polynomial time algorithm for near-optimal reinforcement learning}},
volume = {3},
year = {2001}
}
@article{,
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/reinforcement.pdf:pdf},
title = {{Kearn98}}
}
@article{Zukerman2007,
abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we showthat the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.6028v1},
author = {Zukerman, I and Zukerman, I and Albrecht, D W and Albrecht, D W and Zhou, Li and White, John Myles and Voigtlaender, Paul and Shao, Xuhui and Li, Lexin and Said, Dormouse and Mitchell, T M and Mahajan, Aditya and Li, Wei and Wang, Xuerui and Zhang, Ruofei and Cui, Ying and Mao, Jianchang and Jin, Rong and Li, Cheng and Bendersky, Michael and Garg, Vijay and Ravi, Sujith and Kuleshov, Volodymyr and Precup, D and Kalos, Malvin H. and Whitlock, Paula a. and Jr, Joe F. Hair and Hoffman, Md and Gelman, Andrew and Granville, Vincent and Goul, Michael and Balkan, Sule and Dolk, Daniel and Gentle, James E and Frank, Eibe and Fernandes, Ricardo Felipe and Teixeira, Costa Magalhaes and Dietterich, Tg and Cao, F. and Ester, M. and Qian, W. and Zhou, A. and Cabras, S and Morales, J and Bucklin, Randolph E and Lattin, James M and Ansari, Asim and Gupta, Sunil and Bell, David and Little, John D C and Mela, Carl and Montgomery, Alan and Steckel, Joel and Berman, Ron and Auer, P and Cesa-bianchi, N and Fischer, P and Airlines, American},
doi = {10.1145/1552303.1552307},
eprint = {arXiv:1402.6028v1},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/Zukerman et al. - 2007 - Monte Carlo Methods.pdf:pdf},
isbn = {089871611X | 9780898716115},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {Campaign management support systems,Data handling,Data mining algorithms,Density based clustering,Evolving data streams,Multivariate anal,Predictive analytics,Predictive process,adaptive allocation rules,adaptive monte carlo,all or part of,back,bagged,bandit problems,bayesian inference,bayesian networks,c{\_}and{\_}e,cation,classi,click feed-,click-through-rate,collaborative learning,content-based learning,digital advertising,dual averaging,event discovery,event retrieval,exploitation and exploration,finite horizon regret,generalized pareto distribution,hamiltonian monte carlo,linear models,linear regression,locally weighted regression,markov chain monte carlo,markov models,model trees,models,monte carlo,multi-touch attribution model,naive bayes,neural networks,online advertising,or hard copies of,org entities,partial posterior predictive distribution,permission to make digital,regression,rule induction,schema,student{\_}modeling,tfidf-based,this work for,threshold selection,web events},
number = {1},
pages = {1--123},
pmid = {18292226},
title = {{Monte Carlo Methods}},
url = {http://link.springer.com/article/10.1023/A:1020231107662{\%}5Cnhttp://link.springer.com/chapter/10.1007/3-540-70659-3{\_}2{\%}5Cnhttp://www.datashaping.com/ABbook5.pdf{\%}5Cnhttp://phyusdb.files.wordpress.com/2013/03/monte-carlo-methods-second-revised-and-enlarged-ed},
volume = {1},
year = {2007}
}
@article{Osband2016,
abstract = {Computational results demonstrate that posterior sampling for reinforcement learning (PSRL) dramatically outperforms algorithms driven by optimism, such as UCRL2. We provide insight into the extent of this performance boost and the phenomenon that drives it. We leverage this insight to establish an {\$}\backslashtilde{\{}O{\}}(H\backslashsqrt{\{}SAT{\}}){\$} Bayesian expected regret bound for PSRL in finite-horizon episodic Markov decision processes, where {\$}H{\$} is the horizon, {\$}S{\$} is the number of states, {\$}A{\$} is the number of actions and {\$}T{\$} is the time elapsed. This improves upon the best previous bound of {\$}\backslashtilde{\{}O{\}}(H S \backslashsqrt{\{}AT{\}}){\$} for any reinforcement learning algorithm.},
archivePrefix = {arXiv},
arxivId = {1607.00215},
author = {Osband, Ian and {Van Roy}, Benjamin},
eprint = {1607.00215},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/1607.00215.pdf:pdf},
isbn = {9781510855144},
title = {{Why is Posterior Sampling Better than Optimism for Reinforcement Learning?}},
url = {http://arxiv.org/abs/1607.00215},
year = {2016}
}
@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/1605.09674.pdf:pdf},
issn = {10495258},
title = {{VIME: Variational Information Maximizing Exploration}},
url = {http://arxiv.org/abs/1605.09674},
year = {2016}
}
@article{Osband2013,
abstract = {Most provably-efficient learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration, posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an {\$}\backslashtilde{\{}O{\}}(\backslashtau S \backslashsqrt{\{}AT{\}}){\$} bound on the expected regret, where {\$}T{\$} is time, {\$}\backslashtau{\$} is the episode length and {\$}S{\$} and {\$}A{\$} are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.},
archivePrefix = {arXiv},
arxivId = {1306.0940},
author = {Osband, Ian and Russo, Daniel and {Van Roy}, Benjamin},
eprint = {1306.0940},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/1306.0940.pdf:pdf},
issn = {10495258},
pages = {1--10},
title = {{(More) Efficient Reinforcement Learning via Posterior Sampling}},
url = {http://arxiv.org/abs/1306.0940},
year = {2013}
}
@article{Auer2006,
abstract = {We present a learning algorithm for undiscounted reinforcement learning. $\backslash$r$\backslash$nOur interest lies in bounds for the algorithm's online performance after$\backslash$r$\backslash$nsome finite number of steps. In the spirit of similar methods already successfully applied for the exploration-exploitation tradeoff in multi-armed bandit problems, we use upper confidence bounds to show that our UCRL algorithm achieves logarithmic online regret in the number of steps taken with respect to an optimal policy.},
author = {Auer, Peter},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/UCRL.pdf:pdf},
journal = {Nips},
title = {{Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning}},
year = {2006}
}
@article{Osband2017,
abstract = {We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.},
archivePrefix = {arXiv},
arxivId = {1703.07608},
author = {Osband, Ian and {Van Roy}, Benjamin and Russo, Daniel and Wen, Zheng},
eprint = {1703.07608},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/Generalization and Exploration via Randomized Value Functions.pdf:pdf},
isbn = {9781510829008},
title = {{Deep Exploration via Randomized Value Functions}},
url = {http://arxiv.org/abs/1703.07608},
year = {2017}
}
@article{Kaufmann2018,
abstract = {This paper is about index policies for minimizing (frequentist) regret in a stochastic multi-armed bandit model, inspired by a Bayesian view on the problem. Our main contribution is to prove that the Bayes-UCB algorithm, which relies on quantiles of posterior distributions, is asymptotically optimal when the reward distributions belong to a one-dimensional exponential family, for a large class of prior distributions. We also show that the Bayesian literature gives new insight on what kind of exploration rates could be used in frequentist, UCB-type algorithms. Indeed, approximations of the Bayesian optimal solution or the Finite Horizon Gittins indices provide a justification for the kl-UCB+ and kl-UCB-H+ algorithms, whose asymptotic optimality is also established.},
archivePrefix = {arXiv},
arxivId = {1601.01190},
author = {Kaufmann, Emilie},
doi = {10.1214/17-AOS1569},
eprint = {1601.01190},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/Bayes{\_}UCB.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Bayesian methods,Gittins indices,Multi-armed bandit problems,Upper-confidence bounds},
number = {2},
pages = {842--865},
title = {{On Bayesian index policies for sequential resource allocation}},
volume = {46},
year = {2018}
}
@article{Lattimore2018,
author = {Lattimore, Tor and Szepesv, Csaba},
file = {:home/reazul/Books/Bandits{\_}book{\_}by{\_}Tor.pdf:pdf},
title = {{Bandits Book}},
year = {2018}
}
@article{Jaksch2010,
abstract = {For undiscounted reinforcement learning in Markov decision processes (MDPs) we consider the total regret of a learning algorithm with respect to an optimal policy. In order to describe the transition structure of an MDP we propose a new parameter: An MDP has diameter D if for any pair of states s,s' there is a policy which moves from s to s' in at most D steps (on average). We present a reinforcement learning algorithm with total regret {\~{O}}(DS√AT) after T steps for any unknown MDP with S states, A actions per state, and diameter D. A corresponding lower bound of $\Omega$(√DSAT) on the total regret of any learning algorithm is given as well.$\backslash$r$\backslash$nThese results are complemented by a sample complexity bound on the number of suboptimal steps taken by our algorithm. This bound can be used to achieve a (gap-dependent) regret bound that is logarithmic in T.$\backslash$r$\backslash$nFinally, we also consider a setting where the MDP is allowed to change a fixed number of l times. We present a modification of our algorithm that is able to deal with this setting and show a regret bound of {\~{O}}(l1/3T2/3DS√A).},
archivePrefix = {arXiv},
arxivId = {1403.3741},
author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
eprint = {1403.3741},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/Near-optimal Regret Bounds for Reinforcement Learning.pdf:pdf},
isbn = {Technical Report No. CIT-2009-01},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Computational,Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation,Theory {\&} Algorithms},
number = {1},
pages = {1563--1600},
title = {{Near-optimal Regret Bounds for Reinforcement Learning}},
url = {http://eprints.pascal-network.org/archive/00007081/},
volume = {11},
year = {2010}
}
@article{Kaufmann2012,
author = {Kaufmann, Emilie and Capp, Olivier},
file = {:home/reazul/PhD{\_}Research/Bayesian{\_}Exploration/On Bayesian Upper Confidence Bounds for Bandit Problems.pdf:pdf},
title = {{On Bayesian Upper Confidence Bounds for Bandit Problems}},
year = {2012}
}
