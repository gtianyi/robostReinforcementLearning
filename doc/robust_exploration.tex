% Preamble
% ---

% These are the instructions for authors for IJCAI-18.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
\usepackage{icml2018}
\usepackage{times}  %Required
\usepackage{helvet}  %Required
\usepackage{courier}  %Required
\usepackage{url}  %Required
\usepackage{graphicx}  %Required
\usepackage{titlesec}
\usepackage{mathtools}
\usepackage{comment}
\usepackage[11pt]{moresize}

\usepackage{graphicx}

\makeatletter
\newcommand*\bigcdot{\mathpalette\bigcdot@{.5}}
\newcommand*\bigcdot@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother

\frenchspacing  %Required
\setlength{\pdfpagewidth}{8.5in}  %Required
\setlength{\pdfpageheight}{11in}  %Required
\setcounter{secnumdepth}{2}  
 
\titlespacing*{\paragraph}{0pt}{1.5ex plus 1ex minus .2ex}{1.3ex plus .2ex}
 
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
%\usepackage[small]{caption}
\usepackage{nicefrac}
\usepackage{mathtools}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{caption}
% Packages
% ---
\usepackage{amsmath} % Advanced math typesetting
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathabx}
\usepackage{array}
\usepackage{bbm}
%\usepackage{subcaption}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
%\usepackage{tikz}
%\usepackage{tkz-graph}
\usepackage{calrsfs}
%\usetikzlibrary{shapes,arrows,positioning,calc}
\usepackage[capitalize]{cleveref}

\crefname{algocf}{Algorithm}{Algorithm}
\Crefname{algocf}{Algorithm}{Algorithm}

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\DeclarePairedDelimiter{\abs}{\vert}{\rvert}

\makeatletter
\newcommand{\citeasnoun}[1]{\cite{#1}}
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\setsep}{~:~}
\newcommand{\opt}{^\star}
\newenvironment{mprog}{\begin{array}{>{\displaystyle}l>{\displaystyle}l>{\displaystyle}l}}{\end{array}}
\newcommand{\stc}{\\[1ex]  \mbox{s.t.} &}
\newcommand{\cs}{\\[1ex] & }
\newcommand{\minimize}[1]{\min_{#1} &}
\newcommand{\maximize}[1]{\max_{#1} &}
\newcommand{\tr}{^{\mathsf{T}}}
\newcommand{\one}{\mathbf{1}}
\newcommand{\Real}{\mathbb{R}}
\renewcommand{\ss}{~:~}

\newcommand{\eye}{\mathbf{I}}
\newcommand{\zero}{\mathbf{0}}

\usepackage{natbib}
\bibliographystyle{named}
\setcitestyle{square}

\newcommand{\RNum}[1]{\uppercase\expandafter{\romannumeral #1\relax}}
\newcommand*{\todo}{\textcolor{red}}
\newcommand{\marek}[1]{\textcolor{blue}{[[#1]]}}

\newcommand{\states}{\mathcal{S}}
\newcommand{\actions}{\mathcal{A}}
\newcommand{\mdp}{\mathcal{M}}
\newcommand{\data}{\mathcal{D}}
\newcommand{\pset}{\mathcal{P}}
\newcommand{\tran}{p\opt}
\newcommand{\aset}{\mathcal{P}}
\newcommand{\aseth}{\aset^{H}}
\newcommand{\asetht}{\aset^{T}}
\newcommand{\asetb}{\aset^{B}}
\newcommand{\asetr}{\aset^{R}}
\newcommand{\BU}{T}
\newcommand{\rob}{^R}
\newcommand{\RBU}{\widehat{T}}
\newcommand{\indicator}{\mathbbm{1}}
\newcommand{\vset}{\mathcal{V}}
\newcommand{\vsetr}{\vset^R}

\newcommand{\statecount}{S}
\newcommand{\actioncount}{S}

%\newcommand{\fix}[1]{}
\newcommand{\fix}[1]{{$\langle${\color{red}\sc Fix: #1}$\rangle$}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}

\title{Robust Exploration with Tight Bayesian Plausibility Sets}
%PDF Info Is Required:
\pdfinfo{
	/Tight Bayesian Ambiguity Sets for Robust MDPs
	/author names}

\author{
UNH
}
\date{}
% Main document
% ---
\begin{document}
\maketitle

%\tikzset{
%block/.style = {draw, fill=white, rectangle, minimum height=3em, minimum width=3em},
%tmp/.style  = {coordinate}, 
%sum/.style= {draw, fill=white, circle, node distance=1cm},
%input/.style = {coordinate},
%output/.style= {coordinate},
%pinstyle/.style = {pin edge={to-,thin,black}}
%}

\begin{abstract}
Optimism about the poorly understood states and actions is the main driving force of exploration for many provably-efficient reinforcement learning algorithms. We propose optimism in the face of sensible value functions (OFVF)- a novel reinforcement learning algorithm designed to explore robustly minimizing the worst case exploration cost. OFVF proceeds in an episodic manner, where the duration of the episode is fixed and known. OFVF relaxes the requirement for the set of plausible MDPs to be represented by a confidence interval. It also optimizes the size and location of the plausibility set. Our algorithm is inherently Bayesian and can leverage prior information. Our theoretical analysis shows the robustness of OFVF, and the empirical results demonstrate its practical promise.
\end{abstract}

\section{Introduction}

Markov decision processes (MDPs) provide a versatile methodology for modeling dynamic decision problems under uncertainty~\citep{Bertsekas1996,Sutton1998,Puterman2005}. A perfect MDP model for many reinforcement learning problems is not known precisely in general. Instead, a reinforcement learning agent tries to maximize its cumulative payoff by interacting in an unknown environment with an effort to learn the underlying MDP model. It is important for the agent to exploresub-optimal actions to accelerate the MDP learning task which can help to optimize long-term performance. But it is also important to pick actions with highest known rewards to maximize short-run performance. So the agent always needs to balance between them to boost the performance of a learning algorithm during learning. 

\emph{Optimism in the face of uncertainty (OFU)} is a common principle for most reinforcement learning algorithms encouraging exploration~\citep{Jaksch2010,Brafman2001,Kearns1998a}. The idea is to assign a very high exploration bonus to poorly understood states and actions. The agent chooses a policy under this very "optimistic" model of the environment. As the less understood states-actions are incentivized, they seem lucrative to the agent encouraging exploration. As the agent visits and gathers statistically significant evidence for these states-actions, the uncertainty and optimism decreases converging to reality. Many RL algorithms including \emph{Explicit Explore or Exploit $(E^3)$}~\citep{Kearns1998a}, \emph{R-{\ssmall MAX}}~\cite{Brafman2001}, \emph{UCRL2}~\cite{Auer2006}, \emph{MBIE}~\citep{Strehl2008,Strehl2004,Wiering1998} build on the idea of optimism guiding the exploration. These algorithms provide strong theoretical guarantees with polynomial bound on sample complexity. 

The performance of these OFU algorithms greatly depends on the methods to implement optimism (e.g. Chernoff-Hoeffding's inequality for UCRL2, Confidence Interval for MBIE), which can often be complex in nature. Dealing with a family of plausible environments can sometimes become expensive as well. With OFU exploration, it is possible for an agent to be overly optimistic about a potentially catastrophic situation and end up there paying an extremely high price (e.g. a self driving car hits a wall, a robot falls off the cliff etc.). Exploring and learning such a situation may not payoff the price. It can be wise for the agent to be robust and avoid those situations minimizing the worst-case exploration cost$-$ which we call robust exploration. OFU algorithms are optimistic by definition and cannot guarantee robustness while exploring.

\emph{Probability matching} class of algorithms like \emph{Thompson sampling}~\citep{Thompson1933} performs exploration with a proportional likelihood to the underlying true parameters and has been successfully applied to multi-armed bandit problems~\citep{Agrawal2012,Agrawal2011}. \emph{Posterior Sampling for reinforcement learning (PSRL)}~\citep{Osband2016, Osband2013, Strens2002} applies the same idea in the context of reinforcement learning. PSRL algorithm samples a single instance of the environment from the posterior distribution, then solves and executes the policy optimal for that sampled environment over the episode. Selection of a policy in PSRL is proportional to the probability of that being optimal and exploration is guided by the variance of sampled policies as opposed to optimism. PSRL algorithm is simple, computationally efficient and can utilize any prior structural information to improve exploration. Strong theoretical analysis and practical applications for PSRL are also established in the literature. But similar to OFU algorithms, PSRL cannot handle worst case exploration penalty and performs poorly in such situations.

The main contribution of this paper is OFVF, a new \emph{data-driven} Bayesian approach to constructing \emph{Plausibility} sets for MDPs. The method computes policies with tighter robust estimates for exploration by introducing two new ideas. First, it is based on Bayesian posterior distributions rather than distribution-free bounds. Second, OFVF does not construct plausibility sets as simple confidence intervals. Confidence intervals as plausibility sets are a sufficient but not a necessary condition. OFVF uses the structure of the value function to optimize the \emph{location} and \emph{shape} of the plausibility set to guarantee upper bounds directly without necessarily enforcing the requirement for the set to be a confidence interval.

The paper is organized as follows ...
\section{Problem Statement}

We consider the problem of learning a finite horizon Markov Decision Process $\mdp$ with states $\states = \{1, \ldots, S \}$ and actions $\actions = \{1, \ldots, A \}$. $p: \states \times \actions \to \Delta^\states$ is a transition function, where $p^a_{ss'}$ is interpreted as the probability of ending in state $s'\in\states$ by taking an action $a\in\actions$ from state $s\in\states$. We omit $s'$ when the next state is not deterministic and denote the transition probability as $p_{sa}\in\Real^S$. $R: \states \times \actions \to \Real$ is a reward function and $R^a_{ss'}$ is the reward for taking action $a\in\actions$ from state $s\in\states$ and reaching state $s'\in\states$. Each MDP $\mdp$ is associated with a discount factor $0\le\gamma\le1$ and a distribution of initial state probabilities $p_0$. $L$ is the number of episodes and $H$ is the number of periods in each episode, we consider an episodic learning environment throughout the paper. In each episode $l\in L$, an initial state $s_0\in\states$ is sampled from $p_0$. In period $h=0\ldots H$ of each episode, for a state $s_h$ and action $a_h$, a next state $s_{h+1}$ is sampled following $p^{a_h}_{s_hs_{h+1}}$ and a reward is obtained from $R^{a_h}_{s_hs_{h+1}}$.

A policy $\pi = (\pi_0,\ldots,\pi_{H-1})$ is a set of functions mapping a state $s\in\states$ to an action $a\in\actions$. We define a value function for each policy $\pi$ as:
\begin{equation} \label{eq:state_value}
V^\pi_h(s) := \sum_{s'} P^{\pi(s)}_{ss'} [ r_h + V(s') ]
\end{equation}
\begin{comment}
\begin{equation*}
\begin{split}
V^\pi_h(s) := \E_{\mdp}\bigg[ \sum_{\tau=h}^{H}r_\tau |& s_h=s,a_\tau=\pi_tau(s_\tau) \bigg],\\
&\text{ for } \tau=h,\ldots,H-1
\end{split}
\end{equation*}
\end{comment}
The optimal value function is defined by $V\opt_h(s) = \max_\pi V^\pi_h(s)$ and the optimal policy is defined by $\pi\opt(s) = \arg\max_{a\in\actions} p^a_{ss'}V(s') \text{, } \forall s'\in\states: p^a_{ss'}>0$. We also define the state-action optimal value function for $h=0,\ldots,H-1$ as:
\begin{equation} \label{eq:state_action_value}
Q\opt_h(s,a) := \sum_{s'}P^{a}_{ss'}[r_h + V\opt (s') ]
\end{equation}
%\[
%Q\opt_h(s,a) := \E_{\mdp}\bigg[ r_h+V\opt_{h+1}(s_{h+1}) | s_h=s, a_h=a \bigg]
%\]
The optimal policy $\pi\opt$ is defined as $\pi\opt(s) = \arg\max_{a\in\actions}Q\opt_h(s,a) \text{, } \forall s,h$. Optimistic algorithms encouraging exploration find the probability distribution $\tilde{P}_{sa}$ for each state and action within an interval of the empirically derived distribution $\bar{p}_{sa} = \E[\cdot|s,a]$, which defines the plausible set $\pset_{sa}$ of MDPs. They then solve an optimistic version
of \cref{eq:state_action_value} within $\pset_{sa}$ that leads to the policy with highest reward.
\begin{equation}
Q\opt_h(s,a) := \max_{p^{sa} \in \pset^{sa}} \sum_{s'}p^{a}_{ss'}[r_h + V\opt (s') ]
\end{equation}
An RL agent interacts with the environment in an episodic setting. At each episode $l\in 0,\ldots,L-1$, the agent takes actions based on the policy $\pi\opt_l$ optimal in episode $l$ and realizes reward $\sum_{h=0}^H r_{lh}$. $\pi\opt_l$ is computed from the experiences gathered by the agent in previous episodes $0,\ldots,l-1$. We evaluate the performance of the agent in terms of \emph{expected cumulative regret}:
\[
Regret(T) = \sum_{l=0}^{T/H-1} \E\bigg[ V\opt(s_0) - V^{\pi\opt_l}(s_0) \bigg]
\]
Where $s_0 \sim p_0$, and $V\opt(s_0)$ are the true values of initial states.. 

\section{Interval Estimation for Plausibility Sets}
When solving an MDP in reinforcement learning, the main type of uncertainties are encountered on the model parameters, namely in transition probabilities and rewards. Given data set $\data$, an interval estimation of the model parameters is able to acknowledge and quantify this uncertainty, which a maximum likelihood based point estimate cannot do. In this section, we first describe the standard approach to constructing plausibility sets as distribution free confidence intervals. We then propose its extension to Bayesian setting and present a simple algorithm to serve that purpose. It is important tonote that distribution-free bounds are subtly different from the Bayesian bounds, the Bayesian safety guarantee holds conditional on a given dataset $\data$ while the distribution-free holdacross the sets $\data$. This makes the guarantees qualitatively different and difficult to compare.

\subsection{Plausibility Sets as Confidence Intervals} \label{ssec:freq_pset}
It is common in the literature to use $L_1$ norm as the distribution-free bound. This bound is constructed around the empirical mean of the transition probability $\bar{P}_{s,a}$ by applying the Hoeffding inequality~\citep{Auer2009,Auer2010a,Petrik2016,Wiesemann2013a,Strehl2004}.
\[
\pset^{sa} = \bigg\{ \lVert \tilde{p}_{sa} - \bar{p}_{sa} \rVert_1 \le \sqrt{\frac{2}{n_{s,a}}\log\frac{SA2^S}{\delta}} \bigg\}
\]
where $\bar{P}_{sa}$ is the mean transition computed from D, $n_{s,a}$ is the number of times the agent arrived state $s'$ after taking action $a$ in state $s$, $\delta$ is the required probability of the interval and $\lVert \bigcdot \rVert_1$ is the $L_1$ norm. An important limitation of this approach is that, the size of $\pset_{sa}$ grows linearly with the number of states, which makes it practically useless in general.

\subsection{Bayesian Plausibility Sets} \label{ssec:bayes_pset}
The Bayesian plausibility sets take the same interval estimation idea and extend it into Bayesian setting, which is analogous to \emph{credible intervals} in Bayesian statistics. Credible intervals are constructed with the posterior probability distributions and they are fixed $-$ not a random variable, given the data $\data$. Instead the estimated transition probabilities maximizing the rewards are random variables. They can take advantage of situation-specific prior knowledge unlike general confidence intervals as discussed above. However, a concise comparison between Bayesian and frequentist bounds is beyond the scope of this paper.

The credible region for the plausibility set can be constructed around the mean, median or mode of the sample posterior distribution. Among which, bound around the empirical mean of the transition probabilities is simple and more common. To construct such a region, we optimize for the smallest plausibility set around the mean transition
probability with the assumption that a smaller plausibility set will lead to a tighter upper bound estimate.Formally, the optimization problem to compute $\psi_{s,a}$ for each state s and action a is:
\begin{equation} \label{eq:optimization_bci}
\min_{\psi\in\Real_+} \left\{\psi \ss \P\left[ \norm{\tilde{p}_{s,a} - \bar{p}_{s,a}}_1 > \psi ~|~ \mathcal{D} \right] < \delta \right\}~,
\end{equation}
where nominal point is $\bar{p}_{s,a} = \E_{\tilde{P}}[\tilde{p}_{s,a} ~|~ \mathcal{D}]$. The nominal point is not included in the optimization to reduce computational complexity. 

The optimization problem in \cref{eq:optimization_bci} can be solved by the Sample Average Approximation~(SAA) algorithm~\citep{Shapiro2014}. The main idea is to sample from the posterior distribution and then choose the minimal size $\psi_{s,a}$ that satisfies the constrain.  \Cref{alg:bayes}, in the appendix, summarizes the simple sort-based method.

\subsection{Bayesian Upper Confidence Reinforcement Learning}
\begin{algorithm}
	\KwIn{Prior distribution $\theta^0$ over $p\opt_{s,a}$, desired confidence level $\delta$, number of episodes K}
	\KwOut{Policy with an optimistic return estimate }
	$k=1$,
	$X=\phi$\;
	\Repeat{$k \le K$}{
		
		Initialize MDP: $\mdp^k$\;
		Compute posterior from $\theta^{k-1}$ and samples $X$: $\theta^k$\;
		
		\For{All states and actions: $s \in \mathcal{S}, a \in \mathcal{A}$}{%
			Compute mean transition $\bar{p}_{s,a}$, and interval $\psi_{s,a}$ (Invoke \cref{alg:bayes})\;
			Add transition for $s$ and $a$ in $\mdp^k$\;
		}
		Compute value function and policy: $v_k$, $\hat{\pi}_k$ $\gets$ solve $\mdp^k$\;
		$X\gets$ execute $\hat{\pi}_k$ and collect samples\;
		$k \gets k+1$
	}
	\Return $(\hat{\pi}_k, p_0\tr v_k)$ \;
	\caption{Bayes UCRL}    \label{alg:bayes_ucrl}
\end{algorithm}
We are now ready to describe a simple Bayesian optimistic algorithm, named as Bayes-UCRL. This builds directly on top of UCRL~\citep{Auer2009,Auer2010a}, which constructs the plausibility set as distribution free bounds, similar to what is described in \cref{ssec:freq_pset}. Unlike UCRL, Bayes-UCRL constructs the plausibility set in a Bayesian setting, as described in \cref{ssec:bayes_pset}. \cref{alg:bayes_ucrl} describes the idea in more detail. The algorithm uses a hierarchical Bayesian model that can be used to infer the posterior transition probability over $p\opt$. The details of the Bayesian model are largely irrelevant
for our purpose. The model may have a simple analytical posterior, or may need to be sampled usingMCMC methods like Stan~\citep{Gelman2004}. The algorithm assumes that it is possible to draw enough samples from the posterior that the sampling error becomes negligible. 

\cref{alg:bayes_ucrl} proceeds in an episodic manner. At the beginning of each episode, it computes the posterior transition probabilities for each state and action. It then invokes \cref{alg:bayes} to compute the nominal transition points and the interval estimations for each state and action. It solves the MDP to compute the optimal policy for current episode. The execution of this policy for a fixed horizon generates more samples for posterior estimation in the next episode. This algorithm requires some extra steps compared to UCRL. It is also computationally expensive. But it performs better than UCRL in terms of computed returns over episodes which we will see in \cref{sec:experiments}.

\section{OFVF: Optimism in the Face of sensible Value Functions} \label{sec:multiple}

OFVF uses samples from a posterior distribution, similar to a Bayesian confidence interval, but it relaxes the safety requirement as it is sufficient to guarantee for each state $s$ and action $a$ that:
\begin{equation} \label{eq:condition_safe}
\max_{v\in\mathcal{V}} \P_{P\opt} \left[ \min_{p \in \aset_{s,a}} (p - p_{s,a}\opt)\tr v \le 0  ~\middle|~ \mathcal{D} \right] \ge 1-\delta~,
\end{equation}
with $\mathcal{V} = \{ \hat{v}\opt_{\aset} \}$. To construct the set $\aset$ here, the set $\mathcal{V}$ is not fixed but depends on the robust solution, which in turn depends on $\aset$. RSVF starts with a guess of a small set for $\mathcal{V}$ and then grows it, each time with the current value function, until it contains $\hat{v}\opt_{\aset}$ which is always recomputed after constructing the ambiguity set $\aset$.

\begin{algorithm}
	\KwIn{Desired confidence level $\delta$ and posterior distribution $\P_{P\opt}[\cdot ~|~\mathcal{D}]$ }
	\KwOut{Policy with a maximized safe return estimate }
	Initialize current policy $\pi_0 \gets \arg\max_{\pi} \rho(\pi,\E_{P\opt}[P\opt~|~\mathcal{D}])$\;
	Initialize current value $v_0 \gets v^{\pi_0}_{\E_{P\opt}[P\opt~|~\mathcal{D}]}$\;
	Initialize value robustness set $\mathcal{V}_0 \gets \{v_0 \}$ \;	
	\label{line:make_p_1} Construct $\aset_0$ optimal for $\mathcal{V}_0$\;
	Initialize counter $k\gets 0$\;
	\While{\cref{eq:condition_safe} is violated with $\mathcal{V}=\{v_k\}$}{
		Include $v_k$ that violates \cref{eq:condition_safe}: $\vset_{k+1} \gets \vset_k \cup \{ v_k \}$ \;
		\label{line:make_p_2} Construct $\aset_{k+1}$ optimized for $\vset_{k+1}$\;
		Compute robust value function $v_{k+1}$ and policy $\pi_{k+1}$ for $\aset_{k+1}$\;
		$k \gets k + 1$ \;
	}
	\Return $(\pi_k, p_0\tr v_k)$ \;
	\caption{OFVF: Optimism in the Face of sensible Value Functions}    \label{alg:IAVF}
\end{algorithm} 

In lines 4 and 8 of \cref{alg:IAVF}, $\aset_i$ is computed for each state-action $s,a \in \states\times\actions$. Center $\bar{p}$ and set size $\psi_{s,a}$ are computed from \cref{eq:center_point} using set $\mathcal{V}$ \& optimal $g_v$ computed by solving \cref{eq:optimal_hyperplane}.
When the set $\mathcal{V}$ is a singleton, it is easy to compute a form of an optimal ambiguity set. 
\begin{equation} \label{eq:optimal_hyperplane}
g = \max \left\{ k ~:~ \P_{P\opt} [k \le v\tr p\opt_{s,a}] \ge 1 - \delta/(SA) \right\}
\end{equation}

When $\mathcal{V}$ is a singleton, it is sufficient for the ambiguity set to be a subset of the hyperplane $\{ p \in \Delta^S ~:~ v\tr p = g\opt \}$ for the estimate to be safe. When $\mathcal{V}$ is not a singleton, we only consider the setting when it is discrete, finite, and relatively small. We propose to construct a set defined in terms of an $L_1$ ball with the minimum radius such that it is safe for every $v\in\mathcal{V}$. Assuming that $\mathcal{V} = \{v_1, v_2, \ldots, v_k \}$, we solve the following linear program:
\begin{equation} \label{eq:center_point}
\begin{gathered}
\psi_{s,a} = \min_{p\in\Delta^S} \Bigl\{ \max_{i=1,\ldots,k} \norm{q_i - p}_1 ~:~ 
\hspace{0.1cm}  v_i\tr  q_i = g_i\opt, q_i \in \Delta^S, i \in 1,\ldots,k  \Bigr\}
\end{gathered}
\end{equation}

In other words, we construct the set to minimize its radius while still intersecting the hyperplane for each $v$ in $\mathcal{V}$. \cref{alg:IAVF}, as described, is not guaranteed to converge in finite time as written. It can be readily shown the value functions in the individual iterations are non-increasing. It is easy to just stop once the value function becomes smaller (and that is more conservative) than BCI.

\begin{figure}
	\centering
	\begin{minipage}[c]{.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/gaussian_return_single_state.pdf}\\
	\end{minipage}%
	\begin{minipage}[c]{.45\columnwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/Glossy_Buckthorn_Under_Estimation.pdf}
	\end{minipage}%
	\caption{Return error with a Gaussian prior with 95\% confidence, Left: Single state, Right: Full MDP, X-axis is the number of samples per state-action.}
	\label{fig:return_single_multiple}
\end{figure}

\section{Empirical Evaluation} \label{sec:experiments}

In this section, we evaluate the worst-case estimates computed by Bayes UCRL and OFVF empirically. We assume a true model of each problem and generate a number of simulated data sets for the known distribution. We compute the tightest optimistic estimate for the optimal return and compare it with the optimal return for the true model. We compare our results with ``UCRL2`` and ``PSRL`` algorithms. The value $\xi$ represents the predicted regret, which is the absolute difference between the \emph{true} optimal value and the robust estimate: $\xi = \abs{\rho(\pi\opt_{P\opt}, P\opt) - \hat\rho(\hat\pi\opt)}$, a smaller regret is better. All of our experiments use a 95\% confidence for safety unless otherwise specified.

\subsection{Single-state Bellman Update}
We initially consider simple problems where transition from a single non-terminal state following a single action leads to multiple terminal states. The value function for the terminal states are fixed and assumed to be provided. We evaluate different priors over the transition probabilities: i) uninformative Dirichlet prior and ii) informative Gaussian prior. Note that RSVF is optimal in this simplistic setting, as \cref{fig:dirichlet_result} (left) and \cref{fig:return_single_multiple} (left) shows. As expected, the mean estimate provides the tightest bound, but \cref{fig:dirichlet_result} (right) illustrates that it does not provide any meaningful safety guarantees.

\subsection{River Swim Problem}

\subsection{Mountain Car Problem}


\bibliography{marek,reazul_lib}

\newpage
\appendix
\onecolumn

\section{Technical Results} \label{app:proofs}
\subsection{Computing Bayesian Confidence Interval}

\begin{algorithm}[H]
	\KwIn{Distribution $\theta$ over $p\opt_{s,a}$, confidence level $\delta$, sample count $m$}
	\KwOut{Nominal point $\bar{p}_{s,a}$ and $L_1$ norm size $\psi_{s,a}$}
	Sample $X_1, \ldots, X_m \in \Delta^S$ from $\theta$: $X_i \sim \theta $\;
	Nominal point: $\bar{p}_{s,a} \gets (1/ m) \sum_{i=1}^m X_i $\;
	Compute distances $d_i \gets \lVert \bar{p}_{s,a} - X_i \rVert_1$ and sort \emph{increasingly}\;
	Norm size: $\psi_{s,a} \gets d_{(1-\delta)\,m}$\;
	\Return{$\bar{p}_{s,a}$ and $\psi_{s,a}$}\;
	\caption{Bayesian Confidence Interval (BCI)} \label{alg:bayes}
\end{algorithm}

\end{document}
