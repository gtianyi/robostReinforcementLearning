@book{Ghavamzadeh2015,
	abstract = {Bayesian Reinforcement Learning: A Survey},
	archivePrefix = {arXiv},
	arxivId = {1405.4980},
	author = {Ghavamzadeh, Mohammed and Mannor, Shie and Pineau, Joelle and Tamar, Aviv},
	booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
	doi = {10.1561/2200000049},
	eprint = {1405.4980},
	file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1609.04436.pdf:pdf},
	isbn = {2200000049},
	issn = {1935-8237},
	number = {5-6},
	pages = {359--483},
	pmid = {18255791},
	title = {{Convex Optimization: Algorithms and Complexity}},
	url = {http://www.nowpublishers.com/article/Details/MAL-049},
	volume = {8},
	year = {2015}
}
@article{Satisfaction2013,
author = {Zukui Li, Christodoulos A. Floudas},
doi = {10.1021/ie201651s.A},
file = {:E$\backslash$:/ML-Research/Papers/nihms372641.pdf:pdf},
keywords = {conservatism,probability bounds,probability distribution,robust optimization,tightness},
number = {19},
pages = {6769--6788},
title = {{A Comparative Theoretical and Computational Study on Robust Counterpart Optimization: II. Probabilistic Guarantees on Constraint Satisfaction}},
volume = {51},
year = {2013}
}
@article{Engineering2008,
author = {Katrien R., Gerrit K.},
doi = {10.1504/EJIE.2008.018441},
file = {:E$\backslash$:/ML-Research/Papers/EJIE{\_}R-021-06.pdf:pdf},
number = {May 2014},
title = {{On the choice of a demand distribution for inventory management models}},
year = {2008}
}
@book{Puterman2005,
author = {Puterman, Martin L},
file = {:C$\backslash$:/Users/Reazul Hasan Russel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Puterman - 2005 - Markov decision processes Discrete stochastic dynamic programming.pdf:pdf},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Markov decision processes: Discrete stochastic dynamic programming}},
year = {2005}
}
@book{Eugene2002,
author = {Eugene A. Feinberg, Adam Shwartz},
file = {:C$\backslash$:/Users/Reazul Hasan Russel/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Puterman - 2005 - Markov decision processes Discrete stochastic dynamic programming.pdf:pdf},
publisher = {Springer US},
title = {{Handbook of Markov Decision Processes}},
year = {2002}
}
@article{Kalyanasundaram2002,
author = {Kalyanasundaram, Suresh and Shroff, Ness B},
file = {:E$\backslash$:/ML-Research/Papers/Markov Decision Processes with Uncertain Transition Rates Sensitivity and robust control.pdf:pdf},
isbn = {0780375165},
number = {December},
pages = {3799--3804},
title = {{Markov Decision Processes with Uncertain Transition Rates : Sensitivity and Robust Control}},
year = {2002}
}
@article{Ding2004,
author = {Ding, Chris},
file = {:E$\backslash$:/ML-Research/Papers/KmeansCluesteringViaPCA.pdf:pdf},
title = {{K -means Clustering via Principal Component Analysis}},
year = {2004}
}
@article{Carlier2016,
abstract = {We propose a notion of conditional vector quantile function and a vector quantile regression. A $\backslash$emph{\{}conditional vector quantile function{\}} (CVQF) of a random vector {\$}Y{\$}, taking values in {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$} given covariates {\$}Z=z{\$}, taking values in {\$}\backslashmathbb{\{}R{\}}{\%} {\^{}}k{\$}, is a map {\$}u \backslashlongmapsto Q{\_}{\{}Y\backslashmid Z{\}}(u,z){\$}, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector {\$}U{\$} follows a reference non-atomic distribution {\$}F{\_}U{\$}, for instance uniform distribution on a unit cube in {\$}\backslashmathbb{\{}R{\}}{\^{}}d{\$}, the random vector {\$}Q{\_}{\{}Y\backslashmid Z{\}}(U,z){\$} has the distribution of {\$}Y{\$} conditional on {\$}Z=z{\$}. Moreover, we have a strong representation, {\$}Y = Q{\_}{\{}Y\backslashmid Z{\}}(U,Z){\$} almost surely, for some version of {\$}U{\$}. The $\backslash$emph{\{}vector quantile regression{\}} (VQR) is a linear model for CVQF of {\$}Y{\$} given {\$}Z{\$}. Under correct specification, the notion produces strong representation, {\$}Y=\backslashbeta \backslashleft(U\backslashright) {\^{}}\backslashtop f(Z){\$}, for {\$}f(Z){\$} denoting a known set of transformations of {\$}Z{\$}, where {\$}u \backslashlongmapsto \backslashbeta(u){\^{}}\backslashtop f(Z){\$} is a monotone map, the gradient of a convex function, and the quantile regression coefficients {\$}u \backslashlongmapsto \backslashbeta(u){\$} have the interpretations analogous to that of the standard scalar quantile regression. As {\$}f(Z){\$} becomes a richer class of transformations of {\$}Z{\$}, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge-Kantorovich's optimal transportation problem at its core as a special case. In the classical case, where {\$}Y{\$} is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.},
archivePrefix = {arXiv},
arxivId = {1406.4643},
author = {Carlier, Guillaume and Chernozhukov, Victor and Galichon, Alfred},
doi = {10.1214/15-AOS1401},
eprint = {1406.4643},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/euclid.aos.1460381690.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Monge-Kantorovich-Brenier,Vector conditional quantile function,Vector quantile regression},
number = {3},
pages = {1165--1192},
title = {{Vector quantile regression: An optimal transport approach}},
volume = {44},
year = {2016}
}
@article{Taleghan2015,
abstract = {In a simulator-defined MDP, the Markovian dynamics and rewards are provided in the form of a simulator from which samples can be drawn. This paper studies MDP planning algorithms that attempt to minimize the number of simulator calls before terminating and outputting a policy that is approximately optimal with high probability. The paper introduces two heuristics for efficient exploration and an improved confidence interval that enables earlier termination with probabilis- tic guarantees. We prove that the heuristics and the confidence interval are sound and produce with high probability an approximately optimal policy in polynomial time. Experiments on two benchmark problems and two instances of an invasive species management problem show that the improved confidence intervals and the new search heuristics yield reductions of between 8{\%} and 47{\%} in the number of simulator calls required to reach near-optimal policies.},
author = {Taleghan, Majid Alkaee and Dietterich, Thomas G. and Crowley, Mark and Hall, Kim and Albers, H. Jo},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/taleghan15a.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Good- Turing estimate,MDP planning,Markov decision processes,invasive species management,reinforcement learning},
pages = {3877--3903},
title = {{PAC Optimal MDP Planning with Application to Invasive Species Management}},
url = {http://jmlr.org/papers/v16/taleghan15a.html},
volume = {16},
year = {2015}
}
@article{Kong2012,
abstract = {The use of quantiles to obtain insights about multivariate data is addressed. It is argued that incisive insights can be obtained by considering directional quantiles, the quantiles of projections. Directional quantile envelopes are proposed as a way to condense this kind of information; it is demonstrated that they are essentially halfspace (Tukey) depth levels sets, coinciding for elliptic distributions (in particular multivariate normal) with density contours. Relevant questions concerning their indexing, the possibility of the reverse retrieval of directional quantile information, invariance with respect to affine transformations, and approximation/asymptotic properties are studied. It is argued that the analysis in terms of directional quantiles and their envelopes offers a straightforward probabilistic interpretation and thus conveys a concrete quantitative meaning; the directional definition can be adapted to elaborate frameworks, like estimation of extreme quantiles and directional quantile regression, the regression of depth contours on covariates. The latter facilitates the construction of multivariate growth charts---the question that motivated all the development.},
archivePrefix = {arXiv},
arxivId = {0805.0056},
author = {Kong, Linglong and Mizera, Ivan},
doi = {10.5705/ss.2010.224},
eprint = {0805.0056},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/0805.0056.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {and phrases,data depth,growth charts,quantile regression,quantiles},
number = {2000},
pages = {1--23},
title = {{Quantile tomography: using quantiles with multivariate data}},
url = {http://www3.stat.sinica.edu.tw/statistica/j22n4/J22N410/J22N410.html},
year = {2012}
}
@article{Bertsimas2007,
author = {Bertsimas, Dimitris and Brown, David B},
keywords = {coherent risk measures,robust optimization,spectral risk measures,uncertainty sets},
pages = {1--25},
title = {{Constructing uncertainty sets for robust linear optimization}},
year = {2007}
}
@article{Xu2008,
abstract = {One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms including SVMmap and AdaRank have been proposed and their effectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework for the study and analyze the existing algorithms of SVMmap and AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. Moreover, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We have also conducted comparisons between SVMmap, AdaRank, PermuRank, and conventional methods of Ranking SVM and RankBoost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant difference exists among the performances of the direct optimization methods themselves.},
author = {Xu, Jun and Liu, T Y and Lu, Min and Xu, Jun and Lu, Min and Ma, Wei-Ying and Li, Hang and Li, Hang and Ma, W Y and Liu, Tie-Yan},
doi = {10.1145/1390334.1390355},
isbn = {978-1-60558-164-4},
journal = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {all or part of,evaluation measure,information retrieval,is granted without fee,learning to rank,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
number = {49},
pages = {107--114},
title = {{Directly optimizing evaluation measures in learning to rank}},
url = {http://portal.acm.org/citation.cfm?id=1390334.1390355},
year = {2008}
}
@article{Ball1997,
author = {Ball, Keith},
pages = {1--58},
title = {{An Elementary Introduction to Modern Convex Geometry}},
volume = {31},
year = {1997}
}
@article{Mnih2015,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {learning},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@book{Chains,
author = {Chains, Finite Markov},
isbn = {3540901922},
title = {{Finite Markov Chains}},
volume = {40}
}
@misc{,
title = {lrt{\{}{\_}{\}}framework}
}
@article{Sahni2007,
author = {Sahni, Saurabh},
journal = {Information Retrieval},
title = {{Information Retrieval in Resource Constrained Environments}},
year = {2007}
}
@article{Lovsjo,
author = {Lovsj{\"{o}}, Niclas},
title = {{From Markov chains to Markov decision processes Matematiska institutionen}}
}
@book{Optimization,
author = {Optimization, Robust},
isbn = {9780691143682},
title = {{Robust Optimization}}
}
@article{Kuo2009,
abstract = {Ranking is a key problem in many information retrieval (IR) applications, such as document retrieval and collaborative filtering. In this paper, we address the issue of learning to rank in document retrieval. Learning-based methods, such as RankNet, RankSVM, and RankBoost, try to create ranking functions automatically by using some training data. Recently, several learning to rank methods have been proposed to directly optimize the performance of IR applications in terms of various evaluation measures. They undoubtedly provide statistically significant improvements over conventional methods; however, from the viewpoint of decision-making, most of them do not minimize the Bayes risk of the IR system. In an attempt to fill this research gap, we propose a novel framework that directly optimizes the Bayes risk related to the ranking accuracy in terms of the IR evaluation measures. The results of experiments on the LETOR collections demonstrate that the framework outperforms several existing methods in most cases. Copyright 2009 ACM.},
author = {Kuo, Jen-Wei and Cheng, Pu-Jen and Wang, Hsin-Min},
doi = {10.1145/1645953.1646058},
isbn = {9781605585123},
issn = {1605585122},
journal = {Proceeding of the 18th ACM conference on Information and knowledge management - CIKM '09},
keywords = {learning to rank,ranking function},
pages = {827},
title = {{Learning to rank from Bayesian decision inference}},
url = {http://portal.acm.org/citation.cfm?doid=1645953.1646058},
year = {2009}
}
@article{He2008,
abstract = {Ranking is the key problem for information retrieval and other text applications. Recently, the ranking methods based on machine learning approaches, called learning to rank, become the focus for researchers and practitioners. The main idea of these methods is to apply the various existing and effective algorithms on machine learning to ranking. However, as a learning problem, ranking is different from other classical ones such as classification and regression. In this paper, we investigate the important papers in this direction; the cons and pros of the recent-proposed framework and algorithms for ranking are analyzed, and the relationships among them are discussed. Finally, the promising directions in practice are also pointed out.},
author = {He, Chuan and Wanq, Cong and Zhonq, Yi Xin and Li, Rui Fan},
doi = {10.1109/ICMLC.2008.4620685},
isbn = {9781424420964},
journal = {Proceedings of the 7th International Conference on Machine Learning and Cybernetics, ICMLC},
keywords = {Evaluation,Information retrieval support vector machine,Learning to rank,Ordinal regression,Ranking},
number = {July},
pages = {1734--1739},
title = {{A survey on learning to rank}},
volume = {3},
year = {2008}
}
@article{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
title = {{Reinforcement Learning : An Introduction **** Draft ****}},
year = {2016}
}
@article{,
number = {1},
pages = {1--7},
title = {{Solution Structure for L 1 Uncertainty}}
}
@article{Phophalia2011a,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
number = {December 2011},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Ferns,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
keywords = {1,60j05,68t37,7,90c40,93e20,ams subject classifications,bisimulation,continuous,formalism for describ-,introduction,markov decision process,markov decision processes,mdps,metrics,offer a popular mathematical,planning and learning in,reinforcement learning,the presence of uncertainty,they are a standard,tool for},
number = {1999},
pages = {258--273},
title = {{BISIMULATION METRICS FOR CONTINUOUS MARKOV DECISION}}
}
@article{Jernigan2003,
abstract = {The chi-squared test of Markov chain lumpability is shown to operate reliably under a corrected derivation of the degrees of freedom. The test is used to screen out lumping schemes that corrupt the Markov property and give rise to higher order dependence. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Jernigan, Robert W and Baran, Robert H},
doi = {10.1016/S0167-7152(03)00126-3},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Chi-squared tests,Lumpability,Markov chain,Time series},
number = {1},
pages = {17--23},
title = {{Testing lumpability in Markov chains}},
volume = {64},
year = {2003}
}
@article{Jernigan2003,
abstract = {The chi-squared test of Markov chain lumpability is shown to operate reliably under a corrected derivation of the degrees of freedom. The test is used to screen out lumping schemes that corrupt the Markov property and give rise to higher order dependence. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Jernigan, Robert W and Baran, Robert H},
doi = {10.1016/S0167-7152(03)00126-3},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1-s2.0-S0167715203001263-main.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Chi-squared tests,Lumpability,Markov chain,Time series},
number = {1},
pages = {17--23},
title = {{Testing lumpability in Markov chains}},
volume = {64},
year = {2003}
}
@article{Mnih2015,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {learning},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Petrik2016,
author = {Petrik, Marek},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Petrik2016b.pdf:pdf},
number = {Nips},
title = {{Safe Policy Improvement by Minimizing Robust Baseline Regret}},
year = {2016}
}
@article{Doran2009,
author = {Doran, Christine},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doran - 2009 - ACL-IJCNLP 2009 Handbook.pdf:pdf},
journal = {Presentations},
title = {{ACL-IJCNLP 2009 Handbook}},
year = {2009}
}
@article{Gorissen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.02634v1},
author = {Gorissen, Bram L and Yanıkoğlu, Ihsan and Hertog, Dick Den},
doi = {10.1016/j.omega.2014.12.006.)},
eprint = {arXiv:1501.02634v1},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1501.02634.pdf:pdf},
number = {Soyster 1973},
title = {{A Practical Guide to Robust Optimization}},
year = {2014}
}
@article{Lu2010,
author = {Lu, Tyler and P{\'{a}}l, D{\'{a}}vid and P{\'{a}}l, Martin},
issn = {15324435},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {485--492},
title = {{Contextual multi-armed bandits}},
volume = {9},
year = {2010}
}
@article{Ball1997,
author = {Ball, Keith},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/ball.pdf:pdf},
pages = {1--58},
title = {{An Elementary Introduction to Modern Convex Geometry}},
volume = {31},
year = {1997}
}
@article{Ferns,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/siamFP11.pdf:pdf},
keywords = {1,60j05,68t37,7,90c40,93e20,ams subject classifications,bisimulation,continuous,formalism for describ-,introduction,markov decision process,markov decision processes,mdps,metrics,offer a popular mathematical,planning and learning in,reinforcement learning,the presence of uncertainty,they are a standard,tool for},
number = {1999},
pages = {258--273},
title = {{BISIMULATION METRICS FOR CONTINUOUS MARKOV DECISION}}
}
@article{Gorissen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.02634v1},
author = {Gorissen, Bram L and Yanıkoğlu, Ihsan and Hertog, Dick Den},
doi = {10.1016/j.omega.2014.12.006.)},
eprint = {arXiv:1501.02634v1},
number = {Soyster 1973},
title = {{A Practical Guide to Robust Optimization}},
year = {2014}
}
@article{Petrik2016,
author = {Petrik, Marek},
number = {Nips},
title = {{Safe Policy Improvement by Minimizing Robust Baseline Regret}},
year = {2016}
}
@article{Instructor2009,
author = {Instructor, Combinatorial Optimization and Lecture, Chandra Chekuri and Im, Sungjin},
title = {{1 Polyhedra and Linear Programming}},
year = {2009}
}
@book{Optimizationa,
author = {Optimization, Convex},
isbn = {9780521833783},
title = {{Convex Optimization}}
}
@article{Phillips2006,
abstract = {The availability of detailed environmental data, together with inexpensive and powerful computers, has fueled a rapid increase in predictive modeling of species environmental requirements and geographic distributions. For some species, detailed presence/absence occurrence data are available, allowing the use of a variety of standard statistical techniques. However, absence data are not available for most species. In this paper, we introduce the use of the maximum entropy method (Maxent) for modeling species geographic distributions with presence-only data. Maxent is a general-purpose machine learning method with a simple and precise mathematical formulation, and it has a number of aspects that make it well-suited for species distribution modeling. In order to investigate the efficacy of the method, here we perform a continental-scale case study using two Neotropical mammals: a lowland species of sloth, Bradypus variegatus, and a small montane murid rodent, Microryzomys minutus. We compared Maxent predictions with those of a commonly used presence-only modeling method, the Genetic Algorithm for Rule-Set Prediction (GARP). We made predictions on 10 random subsets of the occurrence records for both species, and then used the remaining localities for testing. Both algorithms provided reasonable estimates of the species' range, far superior to the shaded outline maps available in field guides. All models were significantly better than random in both binomial tests of omission and receiver operating characteristic (ROC) analyses. The area under the ROC curve (AUC) was almost always higher for Maxent, indicating better discrimination of suitable versus unsuitable areas for the species. The Maxent modeling approach can be used in its present form for many applications with presence-only datasets, and merits further research and development.},
author = {Phillips, Steven J. and Anderson, Robert P. and Schapire, Robert E.},
doi = {10.1016/j.ecolmodel.2005.03.026},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions.pdf:pdf},
issn = {03043800},
journal = {Ecological Modelling},
keywords = {Distribution,MaxEnt,Maximum entropy,Modeling,Niche,Range,SDMs},
mendeley-tags = {MaxEnt,SDMs},
month = {jan},
number = {3-4},
pages = {231--259},
title = {{Maximum entropy modeling of species geographic distributions}},
url = {http://www.sciencedirect.com/science/article/pii/S030438000500267X},
volume = {190},
year = {2006}
}
@article{Doran2009,
author = {Doran, Christine},
journal = {Presentations},
title = {{ACL-IJCNLP 2009 Handbook}},
year = {2009}
}
@article{Hasselt,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06461v3},
author = {Hasselt, Hado Van and Guez, Arthur and Silver, David},
eprint = {arXiv:1509.06461v3},
title = {{Deep Reinforcement Learning with Double Q-learning}}
}
@article{Hanasusanto,
author = {Hanasusanto, Grani A and Kuhn, Daniel},
pages = {1--9},
title = {{Robust Data-Driven Dynamic Programming}}
}
@book{Chains,
author = {Chains, Finite Markov},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Kemeny-Snell1976.pdf:pdf},
isbn = {3540901922},
title = {{Finite Markov Chains}},
volume = {40}
}
@article{,
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/robustrectangular{\_}l1.pdf:pdf},
number = {1},
pages = {1--7},
title = {{Solution Structure for L 1 Uncertainty}}
}
@article{,
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/AIMMS3OM{\_}LinearProgrammingTricks.pdf:pdf},
journal = {Technology},
title = {{AIMMS Modeling Guide - Integer Programming Tricks This file contains only one chapter of the book . For a free download of the complete book in pdf format , please visit www.aimms.com or order your hard-}}
}
@article{Hasselt,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06461v3},
author = {Hasselt, Hado Van and Guez, Arthur and Silver, David},
eprint = {arXiv:1509.06461v3},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1509.06461.pdf:pdf},
title = {{Deep Reinforcement Learning with Double Q-learning}}
}
@article{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Sutton, Barto - 2016 - Reinforcement Learning An Introduction.pdf:pdf},
title = {{Reinforcement Learning : An Introduction **** Draft ****}},
year = {2016}
}
@article{Lovsjo,
author = {Lovsj{\"{o}}, Niclas},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/2015{\_}05{\_}report.pdf:pdf},
title = {{From Markov chains to Markov decision processes Matematiska institutionen}}
}
@article{Bertsimas2007,
author = {Bertsimas, Dimitris and Brown, David B},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/5d83b565b9351cdc36b248cde6730dfeaaaf.pdf:pdf},
keywords = {coherent risk measures,robust optimization,spectral risk measures,uncertainty sets},
pages = {1--25},
title = {{Constructing uncertainty sets for robust linear optimization}},
year = {2007}
}
@article{Sahni2007,
author = {Sahni, Saurabh},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahni - 2007 - Information Retrieval in Resource Constrained Environments.pdf:pdf},
journal = {Information Retrieval},
title = {{Information Retrieval in Resource Constrained Environments}},
year = {2007}
}
@book{Optimization,
author = {Optimization, Robust},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/FullBookDec11.pdf:pdf},
isbn = {9780691143682},
title = {{Robust Optimization}}
}
@article{Jarvelin2000,
abstract = {This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is de- sirable from the user point of view in modem large IR envi- ronments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on sepa- rate recall bases for documents of different degrees of rele- vance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of rele- vance. The test was run with a best match retrieval system (In- Query I) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differ- ences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous rele- vance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.},
author = {J{\"{a}}rvelin, Kalervo and Kek{\"{a}}l{\"{a}}inen, Jaana},
doi = {10.1145/345508.345545},
isbn = {1581132263},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '00},
pages = {41--48},
title = {{IR evaluation methods for retrieving highly relevant documents}},
url = {http://dl.acm.org/citation.cfm?id=345508.345545},
year = {2000}
}
@article{Instructor2009,
author = {Instructor, Combinatorial Optimization and Lecture, Chandra Chekuri and Im, Sungjin},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Polyhedra and Linear Programming.pdf:pdf},
title = {{1 Polyhedra and Linear Programming}},
year = {2009}
}
@article{Xu2008,
abstract = {One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms including SVMmap and AdaRank have been proposed and their effectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework for the study and analyze the existing algorithms of SVMmap and AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. Moreover, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We have also conducted comparisons between SVMmap, AdaRank, PermuRank, and conventional methods of Ranking SVM and RankBoost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant difference exists among the performances of the direct optimization methods themselves.},
author = {Xu, Jun and Liu, T Y and Lu, Min and Xu, Jun and Lu, Min and Ma, Wei-Ying and Li, Hang and Li, Hang and Ma, W Y and Liu, Tie-Yan},
doi = {10.1145/1390334.1390355},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2008 - Directly optimizing evaluation measures in learning to rank.pdf:pdf},
isbn = {978-1-60558-164-4},
journal = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {all or part of,evaluation measure,information retrieval,is granted without fee,learning to rank,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
number = {49},
pages = {107--114},
title = {{Directly optimizing evaluation measures in learning to rank}},
url = {http://portal.acm.org/citation.cfm?id=1390334.1390355},
year = {2008}
}
@article{Hanasusanto,
author = {Hanasusanto, Grani A and Kuhn, Daniel},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/NIPS2013{\_}5123.pdf:pdf},
pages = {1--9},
title = {{Robust Data-Driven Dynamic Programming}}
}
@book{Optimizationa,
author = {Optimization, Convex},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/bv{\_}cvxbook.pdf:pdf},
isbn = {9780521833783},
title = {{Convex Optimization}}
}
@article{Kuo2009,
abstract = {Ranking is a key problem in many information retrieval (IR) applications, such as document retrieval and collaborative filtering. In this paper, we address the issue of learning to rank in document retrieval. Learning-based methods, such as RankNet, RankSVM, and RankBoost, try to create ranking functions automatically by using some training data. Recently, several learning to rank methods have been proposed to directly optimize the performance of IR applications in terms of various evaluation measures. They undoubtedly provide statistically significant improvements over conventional methods; however, from the viewpoint of decision-making, most of them do not minimize the Bayes risk of the IR system. In an attempt to fill this research gap, we propose a novel framework that directly optimizes the Bayes risk related to the ranking accuracy in terms of the IR evaluation measures. The results of experiments on the LETOR collections demonstrate that the framework outperforms several existing methods in most cases. Copyright 2009 ACM.},
author = {Kuo, Jen-Wei and Cheng, Pu-Jen and Wang, Hsin-Min},
doi = {10.1145/1645953.1646058},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Cheng, Wang - 2009 - Learning to rank from Bayesian decision inference.pdf:pdf},
isbn = {9781605585123},
issn = {1605585122},
journal = {Proceeding of the 18th ACM conference on Information and knowledge management - CIKM '09},
keywords = {learning to rank,ranking function},
pages = {827},
title = {{Learning to rank from Bayesian decision inference}},
url = {http://portal.acm.org/citation.cfm?doid=1645953.1646058},
year = {2009}
}
@article{GurobiOptimization2014,
author = {{Gurobi Optimization}, Inc.},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Gurobi{\_}quickstart{\_}windows.pdf:pdf},
title = {{Gurobi optimizer quick start guide}},
url = {http://www.gurobi.com/documentation/5.6/quick-start-guide/quickstart.pdf},
year = {2014}
}
@misc{,
title = {lrt{\{}{\_}{\}}framework}
}
@article{Phophalia2011,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
pages = {8--10},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Candes2007,
abstract = {It is now well understood that (1) it is possible to reconstruct sparse signals exactly from what appear to be highly incomplete sets of linear measurements and (2) that this can be done by constrained L1 minimization. In this paper, we study a novel method for sparse signal recovery that in many situations outperforms L1 minimization in the sense that substantially fewer measurements are needed for exact recovery. The algorithm consists of solving a sequence of weighted L1-minimization problems where the weights used for the next iteration are computed from the value of the current solution. We present a series of experiments demonstrating the remarkable performance and broad applicability of this algorithm in the areas of sparse signal recovery, statistical estimation, error correction and image processing. Interestingly, superior gains are also achieved when our method is applied to recover signals with assumed near-sparsity in overcomplete representations--not by reweighting the L1 norm of the coefficient sequence as is common, but by reweighting the L1 norm of the transformed object. An immediate consequence is the possibility of highly efficient data acquisition protocols by improving on a technique known as compressed sensing.},
archivePrefix = {arXiv},
arxivId = {0711.1612},
author = {Candes, Emmanuel J. and Wakin, Michael B. and Boyd, Stephen P.},
doi = {10.1007/s00041-008-9045-x},
eprint = {0711.1612},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/s00041-008-9045-x.pdf:pdf},
isbn = {1069-5869},
issn = {1069-5869},
keywords = {1 -minimization,compressive sensing,dantzig selector,focuss,iterative reweighting,linear equations,sparsity,underdetermined systems of},
pages = {877--905},
pmid = {19110489},
title = {{Enhancing Sparsity by Reweighted L1 Minimization}},
url = {http://arxiv.org/abs/0711.1612},
year = {2007}
}
@article{Gross2016,
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Gross, James J},
doi = {10.1177/0963721414541462.Self-Control},
eprint = {15334406},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/nihms537830.pdf:pdf},
isbn = {0000000000000},
issn = {1527-5418},
keywords = {achievement,another is,but even people who,grit,have comparable levels of,more successful than others,motivation,one obvious answer is,opportunity,self-control,talent,talent and opportunity often,volition,why are some people},
number = {5},
pages = {352--359},
pmid = {24655651},
title = {{HHS Public Access}},
volume = {34},
year = {2016}
}
@article{Szepesvari2010,
abstract = {Reinforcement learning is a learning paradigm concerned with learning to control a system so as to maximize a numerical performance measure that expresses a long-term objective.What distin- guishes reinforcement learning from supervised learning is that only partial feedback is given to the learner about the learner's predictions. Further, the predictions may have long term effects through influencing the future state of the controlled system. Thus, time plays a special role. The goal in reinforcement learning is to develop efficient learning algorithms, as well as to understand the al- gorithms' merits and limitations. Reinforcement learning is of great interest because of the large number of practical applications that it can be used to address, ranging from problems in artificial intelligence to operations research or control engineering. In this book,we focus on those algorithms of reinforcement learning that build on the powerful theory of dynamic programming.We give a fairly comprehensive catalog of learning problems, describe the core ideas, note a large number of state of the art algorithms, followed by the discussion of their theoretical properties and limitations.},
author = {Szepesv{\'{a}}ri, Csaba and Bartok, Gabor},
doi = {10.2200/S00268ED1V01Y201005AIM009},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/RLAlgsInMDPs.pdf:pdf},
isbn = {9781608454921},
issn = {1939-4608},
journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
number = {x},
pages = {1--103},
title = {{Algorithms for Reinforcement Learning (Errata)}},
volume = {4},
year = {2010}
}
@article{Ahmed2017,
author = {Ahmed, Asrar and Jaillet, Patrick},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/SamplingBasedApproachesForMinimizingRegretInUncertainMDP.pdf:pdf},
pages = {229--264},
title = {{Sampling Based Approaches for Minimizing Regret in Uncertain Markov Decision Processes ( MDPs )}},
volume = {59},
year = {2017}
}
@misc{Berthomieu1983,
abstract = {This paper is concerned with specifying and proving correct systems in which time appears as a parameter. We model such systems via Merlin's Time Petri Nets. An enumerative analysis technique is introduced for these nets based on the computation of a set of state classes and a reachability relation on the set. State classes are de ned in the text and an algorithm is provided for their enumeration. This enumerative approach allows us to derive a nite representation of their behavior for a large family of Time Petri Nets. The analysis method is illustrated by the analysis of a communication protocol},
archivePrefix = {arXiv},
arxivId = {arXiv:0912.0827v5},
author = {Berthomieu, B. and Menasche, M.},
booktitle = {Proceedings IFIP},
doi = {10.1007/3-540-45014-9},
eprint = {arXiv:0912.0827v5},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/10.1.1.470.9161.pdf:pdf},
isbn = {0-9695338-5-3},
issn = {0010-4485},
pages = {41--46},
pmid = {12185262},
title = {{An enumerative approach for analyzing time Petri nets}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.4063},
year = {1983}
}
@article{Antonov2013,
author = {Antonov, Anton},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Quantile regression through linear programming.pdf:pdf},
pages = {1--21},
title = {{Quantile regression through linear programming}},
year = {2013}
}
@article{MaxJaderbergVolodymyrMnihWojciechMarianCzarneckiTomSchaulJoelZLeibo2016,
abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the- art on Atari, averaging 880{\%} expert human performance, and a challenging suite of first-person, three-dimensional Labyrinth tasks leading to a mean speedup in learning of 10× and averaging 87{\%} expert human performance on Labyrinth.},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03044v2},
author = {{Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki Tom Schaul, Joel Z Leibo}, David Silver {\&} Koray Kavukcuoglu},
doi = {10.1051/0004-6361/201527329},
eprint = {arXiv:1509.03044v2},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1611.05397.pdf:pdf},
isbn = {2004012439},
issn = {0004-6361},
journal = {arXiv},
pages = {1--11},
pmid = {23459267},
title = {{Reinforcement Learning with Unsupervised Auxiliary Tasks}},
year = {2016}
}
@book{Optimization,
author = {Optimization, Robust},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/FullBookDec11.pdf:pdf},
isbn = {9780691143682},
title = {{Robust Optimization}}
}
@article{Instructor2009,
author = {Instructor, Combinatorial Optimization and Lecture, Chandra Chekuri and Im, Sungjin},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Polyhedra and Linear Programming.pdf:pdf},
title = {{1 Polyhedra and Linear Programming}},
year = {2009}
}
@article{Lovsjo,
author = {Lovsj{\"{o}}, Niclas},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/2015{\_}05{\_}report.pdf:pdf},
title = {{From Markov chains to Markov decision processes Matematiska institutionen}}
}
@book{Optimizationa,
author = {Optimization, Convex},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/bv{\_}cvxbook.pdf:pdf},
isbn = {9780521833783},
title = {{Convex Optimization}}
}
@article{Jernigan2003,
abstract = {The chi-squared test of Markov chain lumpability is shown to operate reliably under a corrected derivation of the degrees of freedom. The test is used to screen out lumping schemes that corrupt the Markov property and give rise to higher order dependence. ?? 2003 Elsevier B.V. All rights reserved.},
author = {Jernigan, Robert W. and Baran, Robert H.},
doi = {10.1016/S0167-7152(03)00126-3},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1-s2.0-S0167715203001263-main.pdf:pdf},
issn = {01677152},
journal = {Statistics and Probability Letters},
keywords = {Chi-squared tests,Lumpability,Markov chain,Time series},
number = {1},
pages = {17--23},
title = {{Testing lumpability in Markov chains}},
volume = {64},
year = {2003}
}
@article{Bertsimas2007,
author = {Bertsimas, Dimitris and Brown, David B},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/5d83b565b9351cdc36b248cde6730dfeaaaf.pdf:pdf},
keywords = {coherent risk measures,robust optimization,spectral risk measures,uncertainty sets},
pages = {1--25},
title = {{Constructing uncertainty sets for robust linear optimization}},
year = {2007}
}
@article{Sutton2016,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Sutton, Barto - 2016 - Reinforcement Learning An Introduction.pdf:pdf},
title = {{Reinforcement Learning : An Introduction **** Draft ****}},
year = {2016}
}
@article{Ball1997,
author = {Ball, Keith},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/ball.pdf:pdf},
pages = {1--58},
title = {{An Elementary Introduction to Modern Convex Geometry}},
volume = {31},
year = {1997}
}
@article{Petrik2016,
author = {Petrik, Marek},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Petrik2016b.pdf:pdf},
number = {Nips},
title = {{Safe Policy Improvement by Minimizing Robust Baseline Regret}},
year = {2016}
}
@article{Hanasusanto,
author = {Hanasusanto, Grani A and Kuhn, Daniel},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/NIPS2013{\_}5123.pdf:pdf},
pages = {1--9},
title = {{Robust Data-Driven Dynamic Programming}}
}
@article{Hasselt,
archivePrefix = {arXiv},
arxivId = {arXiv:1509.06461v3},
author = {Hasselt, Hado Van and Guez, Arthur and Silver, David},
eprint = {arXiv:1509.06461v3},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1509.06461.pdf:pdf},
title = {{Deep Reinforcement Learning with Double Q-learning}}
}
@article{Ferns,
author = {Ferns, Norm and Panangaden, Prakash and Precup, Doina},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/siamFP11.pdf:pdf},
keywords = {1,60j05,68t37,7,90c40,93e20,ams subject classifications,bisimulation,continuous,formalism for describ-,introduction,markov decision process,markov decision processes,mdps,metrics,offer a popular mathematical,planning and learning in,reinforcement learning,the presence of uncertainty,they are a standard,tool for},
number = {1999},
pages = {258--273},
title = {{BISIMULATION METRICS FOR CONTINUOUS MARKOV DECISION}}
}
@article{Gorissen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1501.02634v1},
author = {Gorissen, Bram L and Yanıkoğlu, Ihsan and Hertog, Dick Den},
doi = {10.1016/j.omega.2014.12.006.)},
eprint = {arXiv:1501.02634v1},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/1501.02634.pdf:pdf},
number = {Soyster 1973},
title = {{A Practical Guide to Robust Optimization}},
year = {2014}
}
@book{Chains,
author = {Chains, Finite Markov},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/Kemeny-Snell1976.pdf:pdf},
isbn = {3540901922},
title = {{Finite Markov Chains}},
volume = {40}
}
@article{,
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/robustrectangular{\_}l1.pdf:pdf},
number = {1},
pages = {1--7},
title = {{Solution Structure for L 1 Uncertainty}}
}
@article{Mnih2015,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:home/reazul/Summer-2017/ML+Summer/Summer-2017/Robust{\_}Papers{\_}Summer{\_}17/DQN.pdf:pdf},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {learning},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{Apr2017,
author = {Apr, No Mar and Bean, James C and Birge, John R and Smith, Robert L},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apr et al. - 2017 - Aggregation in Dynamic Programming Author ( s ) James C . Bean , John R . Birge and Robert L . Smith Published by I.pdf:pdf},
number = {2},
pages = {215--220},
title = {{Aggregation in Dynamic Programming Author ( s ): James C . Bean , John R . Birge and Robert L . Smith Published by : INFORMS Stable URL : http://www.jstor.org/stable/170693 REFERENCES Linked references are available on JSTOR for this article : You may nee}},
volume = {35},
year = {2017}
}
@article{VanRoy2006,
abstract = {We consider approximate value iteration with a parameterized approximator in which the state space is partitioned and the optimal cost-to-go function over each partition is approximated by a constant. We establish performance loss bounds for policies derived from approximations associated with fixed points. These bounds identify benefits to using invariant distributions of appropriate policies as projection weights. Such projection weighting relates to what is done by temporal-difference learning. Our analysis also leads to the first performance loss bound for approximate value iteration with an average-cost objective.},
author = {{Van Roy}, Benjamin},
doi = {10.1287/moor.1060.0188},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Roy - 2006 - Performance Loss Bounds for Approximate Value Iteration with State Aggregation.pdf:pdf},
isbn = {0364-765X},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
keywords = {2004,2005,68t05,68t37,90c39,90c40,approximate value iteration,dynamic programming,finite state,history,markov,ms subject classification,msc2000 subject classification,optimal control,or,primary,received august 2,revised august 12,secondary,state aggregation,temporal-difference learning},
number = {2},
pages = {234--244},
title = {{Performance Loss Bounds for Approximate Value Iteration with State Aggregation}},
url = {http://dx.doi.org/10.1287/moor.1060.0188},
volume = {31},
year = {2006}
}
@article{Lim,
author = {Lim, Shiau Hong and Xu, Huan and Mannor, Shie},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim, Xu, Mannor - Unknown - Reinforcement Learning in Robust Markov Decision Processes.pdf:pdf},
pages = {1--9},
title = {{Reinforcement Learning in Robust Markov Decision Processes}}
}
@article{Hutter2014,
abstract = {We consider a Reinforcement Learning setup where an agent interacts with an environment in observation-reward-action cycles without any (esp.$\backslash$ MDP) assumptions on the environment. State aggregation and more generally feature reinforcement learning is concerned with mapping histories/raw-states to reduced/aggregated states. The idea behind both is that the resulting reduced process (approximately) forms a small stationary finite-state MDP, which can then be efficiently solved or learnt. We considerably generalize existing aggregation results by showing that even if the reduced process is not an MDP, the (q-)value functions and (optimal) policies of an associated MDP with same state-space size solve the original problem, as long as the solution can approximately be represented as a function of the reduced states. This implies an upper bound on the required state space size that holds uniformly for all RL problems. It may also explain why RL algorithms designed for MDPs sometimes perform well beyond MDPs.},
archivePrefix = {arXiv},
arxivId = {1407.3341},
author = {Hutter, Marcus},
eprint = {1407.3341},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hutter - 2014 - Extreme State Aggregation Beyond MDPs.pdf:pdf},
keywords = {non-mdp,reinforcement learning,state aggregation},
title = {{Extreme State Aggregation Beyond MDPs}},
url = {http://arxiv.org/abs/1407.3341},
year = {2014}
}
@article{Mastin2012,
abstract = {We analyze losses resulting from uncertain transition probabilities in Markov decision processes with bounded nonnegative rewards. We assume that policies are precomputed using exact dynamic programming with the estimated transition probabilities, but the system evolves according to different, true transition probabilities. Given a bound on the total variation error of estimated transition probability distributions, we derive upper bounds on the loss of expected total reward. The approach analyzes the growth of errors incurred by stepping backwards in time while precomputing value functions, which requires bounding a multilinear program. Loss bounds are given for the finite horizon undiscounted, finite horizon discounted, and infinite horizon discounted cases, and a tight example is shown.},
author = {Mastin, Andrew and Jaillet, Patrick},
doi = {10.1109/CDC.2012.6426504},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mastin, Jaillet - 2012 - Loss bounds for uncertain transition probabilities in Markov decision processes.pdf:pdf},
isbn = {978-1-4673-2066-5},
issn = {01912216},
journal = {Proceedings of the IEEE Conference on Decision and Control},
pages = {6708--6715},
title = {{Loss bounds for uncertain transition probabilities in Markov decision processes}},
year = {2012}
}
@article{Wiesemann2013,
author = {Wiesemann, Wolfram},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiesemann - 2013 - Robust Markov decision processes.pdf:pdf},
journal = {{\ldots} of Operations Research},
keywords = {markov decision processes,robust optimization,semidefinite programming},
pages = {1--52},
title = {{Robust Markov decision processes}},
url = {http://mor.journal.informs.org/content/38/1/153.short},
year = {2013}
}
@article{Nilim2005,
abstract = {Optimal solutions to Markov decision problems may be very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of these probabilities is far from accurate. Hence, estimation errors are limiting factors in applying Markov decision processes to real-world problems. We consider a robust control problem for a finite-state, finite-action Markov decision process, where uncertainty on the transition matrices is described in terms of possibly nonconvex sets. We show that perfect duality holds for this problem, and that as a consequence, it can be solved with a variant of the classical dynamic programming algorithm, the “robust dynamic programming” algorithm. We show that a particular choice of the uncertainty sets, involving likelihood regions or entropy bounds, leads to both a statistically accurate representation of uncertainty, and a complexity of the robust recursion that is almost the same as that of the classical recursion. Hence, robustness can be added at practically no extra computing cost. We derive similar results for other uncertainty sets, including one with a finite number of possible values for the transition matrices. We describe in a practical path planning example the benefits of using a robust strategy instead of the classical optimal strategy; even if the uncertainty level is only crudely guessed, the robust strategy yields a much better worst-case expected travel time.},
author = {Nilim, Arnab and {El Ghaoui}, Laurent},
doi = {10.1287/opre.1050.0216},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nilim, El Ghaoui - 2005 - Robust Control of Markov Decision Processes with Uncertain Transition Matrices.pdf:pdf},
isbn = {0030364X},
issn = {0030-364X},
journal = {Operations Research},
number = {5},
pages = {780--798},
title = {{Robust Control of Markov Decision Processes with Uncertain Transition Matrices}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1050.0216},
volume = {53},
year = {2005}
}
@article{Bagnell2001,
abstract = {The authors consider the fundamental problem of nding good policies in uncertain models. It is demonstrated that although the general problem of nding the best policy with respect to the worst model is NP-hard, in the special case of a convex uncertainty set the problem is tractable. A stochastic dynamic game is proposed, and the security equilibrium solution of the game is shown to correspond to the value function under the worst model and the optimal controller. The authors demonstrate that the uncertain model approach can be used to solve a class of nearly Markovian Decision Problems, providing lower bounds on performance in stochastic models with higher-order interactions. The framework considered establishes connections between and generalizes paradigms of stochastic optimal, mini-max, and H1 /robust control. Applications are considered, including robustness in reinforcement learning, planning in nearly Markovian decision processes, and bounding error due to sensor discretization in noisy, continuous state-spaces.},
author = {Bagnell, J Andrew and Ng, Andrew Y and Schneider, Jeff G},
doi = {tech. report CMU-RI-TR-01-25},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bagnell, Ng, Schneider - 2001 - Solving Uncertain Markov Decision Processes.pdf:pdf},
journal = {Carnegie Mellon Research Showcase},
pages = {948--957},
title = {{Solving Uncertain Markov Decision Processes}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.8550{\%}7B{\%}5C{\&}{\%}7Drep=rep1{\%}7B{\%}5C{\&}{\%}7Dtype=pdf{\%}7B{\%}5C{\%}25{\%}7D5Cnhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.57.8550},
year = {2001}
}
@article{Nilim2004,
abstract = {Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to realworld problems. We propose an algorithm for solving finite-state and finite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efficient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost. 1},
author = {Nilim, Arnab and Ghaoui, Laurent El},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nilim, Ghaoui - 2004 - Robust solutions to Markov decision problems with uncertain transition matrices.pdf:pdf},
journal = {Operations Research},
number = {5},
pages = {780},
title = {{Robust solutions to Markov decision problems with uncertain transition matrices}},
volume = {53},
year = {2004}
}
@article{Ren2002,
author = {Ren, Zhiyuan and Krogh, B.H.},
doi = {10.1109/CDC.2002.1184960},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren, Krogh - 2002 - State aggregation in Markov decision processes.pdf:pdf},
isbn = {0-7803-7516-5},
issn = {01912216},
journal = {Proceedings of the 41st IEEE Conference on Decision and Control, 2002.},
keywords = {cesses,policy iterations,state},
number = {December},
pages = {3819--3824},
title = {{State aggregation in Markov decision processes}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1184960},
volume = {4},
year = {2002}
}
@article{LeTallec2007,
abstract = {Markov Decision Processes (MDPs) model problems of sequential decision-making under uncertainty. They have been studied and applied extensively. Nonetheless, there are two major barriers that still hinder the applicability of MDPs to many more practical decision making problems: * The decision maker is often lacking a reliable MDP model. Since the results obtained by dynamic programming are sensitive to the assumed MDP model, their relevance is challenged by model uncertainty. * The structural and computational results of dynamic programming (which deals with expected performance) have been extended with only limited success to accommodate risk-sensitive decision makers. In this thesis, we investigate two ways of dealing with uncertain MDPs and we develop a new connection between robust control of uncertain MDPs and risk-sensitive control of dynamical systems. The first approach assumes a model of model uncertainty and formulates the control of uncertain MDPs as a problem of decision-making under (model) uncertainty. We establish that most formulations are at least NP-hard and thus suffer from the "'curse of uncertainty." The worst-case control of MDPs with rectangular uncertainty sets is equivalent to a zero-sum game between the controller and nature. The structural and computational results for such games make this formulation appealing. By adding a penalty for unlikely parameters, we extend the formulation of worst-case control of uncertain MDPs and mitigate its conservativeness. We show a duality between the penalized worst-case control of uncertain MDPs with rectangular uncertainty and the minimization of a Markovian dynamically consistent convex risk measure of the sample cost. This notion of risk has desirable properties for multi-period decision making, including a new Markovian property that we introduce and motivate. This Markovian property is critical in establishing the equivalence between minimizing some risk measure of the sample cost and solving a certain zero-sum Markov game between the decision maker and nature, and to tackling infinite-horizon problems. An alternative approach to dealing with uncertain MDPs, which avoids the curse of uncertainty, is to exploit directly observational data. Specifically, we estimate the expected performance of any given policy (and its gradient with respect to certain policy parameters) from a training set comprising observed trajectories sampled under a known policy. We propose new value (and value gradient) estimators that are unbiased and have low training set to training set variance. We expect our approach to outperform competing approaches when there are few system observations compared to the underlying MDP size, as indicated by numerical experiments.},
author = {{Le Tallec}, Yann},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le Tallec - 2007 - Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Processes.pdf:pdf},
journal = {Thesis},
pages = {211},
title = {{Robust, Risk-Sensitive, and Data-driven Control of Markov Decision Processes}},
url = {http://hdl.handle.net/1721.1/38598},
year = {2007}
}
@article{Iyengar2005,
abstract = {In this paper we propose a robust formulation for discrete time dynamic programming (DP). The objective of the robust formulation is to systematically mitigate the sensitivity of the DP optimal policy to ambiguity in the underlying transition probabilities. The ambiguity is modeled by associating a set of conditional measures with each state-action pair. Consequently, in the robust formulation each policy has a set of measures associated with it. We prove that when this set of measures has a certain “rectangularity” property, all of the main results for finite and infinite horizon DP extend to natural robust counterparts. We discuss techniques from Nilim and El Ghaoui [17] for constructing suitable sets of conditional measures that allow one to efficiently solve for the optimal robust policy. We also show that robust DP is equivalent to stochastic zero-sum games with perfect information.},
author = {Iyengar, Garud N.},
doi = {10.1287/moor.1040.0129},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Iyengar - 2005 - Robust Dynamic Programming.pdf:pdf},
isbn = {0364765X},
issn = {0364-765X},
journal = {Mathematics of Operations Research},
number = {2},
pages = {257--280},
title = {{Robust Dynamic Programming}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/moor.1040.0129},
volume = {30},
year = {2005}
}
@article{Freund2003,
abstract = {We study the problem of learning to accurately rank a set of objects by combining a given collec-tion of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the " collaborative-filtering " problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efficient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm's behavior both on the training data, and on new test data not seen during training. We also describe an efficient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the first exper-iment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-filtering task for making movie recommendations.},
author = {Freund, Yoav and Iyer, Raj and Schapire, Robert E and Singer, Yoram},
doi = {10.1162/jmlr.2003.4.6.933},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Freund et al. - 2003 - An Efficient Boosting Algorithm for Combining Preferences.pdf:pdf},
isbn = {1581134924},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {933--969},
pmid = {345},
title = {{An Efficient Boosting Algorithm for Combining Preferences}},
volume = {4},
year = {2003}
}
@article{Burges2005,
abstract = {We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine.},
author = {Burges, Chris and Shaked, Tal and Renshaw, Erin and Lazier, Ari and Deeds, Matt and Hamilton, Nicole and Hullender, Greg},
doi = {10.1145/1102351.1102363},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges et al. - 2005 - Learning to rank using gradient descent.pdf:pdf},
isbn = {1595931805},
issn = {00243205},
journal = {Proceedings of the 22nd international conference on Machine learning - ICML '05},
keywords = {gradient descent,internet search,neural networks,probabilistic cost functions,ranking},
pages = {89--96},
pmid = {16483612},
title = {{Learning to rank using gradient descent}},
url = {http://dl.acm.org/citation.cfm?id=1102351.1102363{\%}5Cnhttp://portal.acm.org/citation.cfm?doid=1102351.1102363},
year = {2005}
}
@article{Sahni2007,
author = {Sahni, Saurabh},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahni - 2007 - Information Retrieval in Resource Constrained Environments.pdf:pdf},
journal = {Information Retrieval},
title = {{Information Retrieval in Resource Constrained Environments}},
year = {2007}
}
@article{Doran2009,
author = {Doran, Christine},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doran - 2009 - ACL-IJCNLP 2009 Handbook.pdf:pdf},
journal = {Presentations},
title = {{ACL-IJCNLP 2009 Handbook}},
year = {2009}
}
@article{Joachims2002,
abstract = {This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.},
author = {Joachims, Thorsten},
doi = {10.1145/775047.775067},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 2002 - Optimizing search engines using clickthrough data.pdf:pdf},
isbn = {158113567X},
issn = {10468188},
journal = {Kdd '02},
keywords = {analysis,information,learning,log,rank,retrieval},
pages = {133--142},
pmid = {21474660},
title = {{Optimizing search engines using clickthrough data}},
url = {http://www.cs.cornell.edu/People/tj/publications/joachims{\_}02c.pdf},
year = {2002}
}
@misc{,
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - herbrich-ordinal-00.gz:gz},
title = {herbrich-ordinal-00}
}
@misc{,
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - lrt{\_}framework.png:png},
title = {lrt{\_}framework}
}
@article{Kuo2009,
abstract = {Ranking is a key problem in many information retrieval (IR) applications, such as document retrieval and collaborative filtering. In this paper, we address the issue of learning to rank in document retrieval. Learning-based methods, such as RankNet, RankSVM, and RankBoost, try to create ranking functions automatically by using some training data. Recently, several learning to rank methods have been proposed to directly optimize the performance of IR applications in terms of various evaluation measures. They undoubtedly provide statistically significant improvements over conventional methods; however, from the viewpoint of decision-making, most of them do not minimize the Bayes risk of the IR system. In an attempt to fill this research gap, we propose a novel framework that directly optimizes the Bayes risk related to the ranking accuracy in terms of the IR evaluation measures. The results of experiments on the LETOR collections demonstrate that the framework outperforms several existing methods in most cases. Copyright 2009 ACM.},
author = {Kuo, Jen-Wei and Cheng, Pu-Jen and Wang, Hsin-Min},
doi = {10.1145/1645953.1646058},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Cheng, Wang - 2009 - Learning to rank from Bayesian decision inference.pdf:pdf},
isbn = {9781605585123},
issn = {1605585122},
journal = {Proceeding of the 18th ACM conference on Information and knowledge management - CIKM '09},
keywords = {learning to rank,ranking function},
pages = {827},
title = {{Learning to rank from Bayesian decision inference}},
url = {http://portal.acm.org/citation.cfm?doid=1645953.1646058},
year = {2009}
}
@article{Transformation,
author = {Transformation, The},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Transformation - Unknown - Retrieval of Information by Computer.pdf:pdf},
title = {{Retrieval of Information by Computer}}
}
@article{Kuo2009,
abstract = {Ranking is a key problem in many information retrieval (IR) applications, such as document retrieval and collaborative filtering. In this paper, we address the issue of learning to rank in document retrieval. Learning-based methods, such as RankNet, RankSVM, and RankBoost, try to create ranking functions automatically by using some training data. Recently, several learning to rank methods have been proposed to directly optimize the performance of IR applications in terms of various evaluation measures. They undoubtedly provide statistically significant improvements over conventional methods; however, from the viewpoint of decision-making, most of them do not minimize the Bayes risk of the IR system. In an attempt to fill this research gap, we propose a novel framework that directly optimizes the Bayes risk related to the ranking accuracy in terms of the IR evaluation measures. The results of experiments on the LETOR collections demonstrate that the framework outperforms several existing methods in most cases. Copyright 2009 ACM.},
author = {Kuo, Jen-Wei and Cheng, Pu-Jen and Wang, Hsin-Min},
doi = {10.1145/1645953.1646058},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Cheng, Wang - 2009 - Learning to rank from Bayesian decision inference.pdf:pdf},
isbn = {9781605585123},
issn = {1605585122},
journal = {Proceeding of the 18th ACM conference on Information and knowledge management - CIKM '09},
keywords = {learning to rank,ranking function},
pages = {827},
title = {{Learning to rank from Bayesian decision inference}},
url = {http://portal.acm.org/citation.cfm?doid=1645953.1646058},
year = {2009}
}
@article{Xu2008,
abstract = {One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms including SVMmap and AdaRank have been proposed and their effectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework for the study and analyze the existing algorithms of SVMmap and AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. Moreover, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We have also conducted comparisons between SVMmap, AdaRank, PermuRank, and conventional methods of Ranking SVM and RankBoost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant difference exists among the performances of the direct optimization methods themselves.},
author = {Xu, Jun and Liu, T.Y. and Lu, Min and Xu, Jun and Lu, Min and Ma, Wei-Ying and Li, Hang and Li, Hang and Ma, W.Y. and Liu, Tie-Yan},
doi = {10.1145/1390334.1390355},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2008 - Directly optimizing evaluation measures in learning to rank.pdf:pdf},
isbn = {978-1-60558-164-4},
journal = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {all or part of,evaluation measure,information retrieval,is granted without fee,learning to rank,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
number = {49},
pages = {107--114},
title = {{Directly optimizing evaluation measures in learning to rank}},
url = {http://portal.acm.org/citation.cfm?id=1390334.1390355},
year = {2008}
}
@article{He2008,
abstract = {Ranking is the key problem for information retrieval and other text applications. Recently, the ranking methods based on machine learning approaches, called learning to rank, become the focus for researchers and practitioners. The main idea of these methods is to apply the various existing and effective algorithms on machine learning to ranking. However, as a learning problem, ranking is different from other classical ones such as classification and regression. In this paper, we investigate the important papers in this direction; the cons and pros of the recent-proposed framework and algorithms for ranking are analyzed, and the relationships among them are discussed. Finally, the promising directions in practice are also pointed out.},
author = {He, Chuan and Wanq, Cong and Zhonq, Yi Xin and Li, Rui Fan},
doi = {10.1109/ICMLC.2008.4620685},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2008 - A survey on learning to rank.pdf:pdf},
isbn = {9781424420964},
journal = {Proceedings of the 7th International Conference on Machine Learning and Cybernetics, ICMLC},
keywords = {Evaluation,Information retrieval support vector machine,Learning to rank,Ordinal regression,Ranking},
number = {July},
pages = {1734--1739},
title = {{A survey on learning to rank}},
volume = {3},
year = {2008}
}
@article{Phophalia2011a,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phophalia - 2011 - A survey on Learning to Rank (LETOR) approaches in information retrieval(2).pdf:pdf},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
number = {December 2011},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Doran2009,
author = {Doran, Christine},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Doran - 2009 - ACL-IJCNLP 2009 Handbook.pdf:pdf},
journal = {Presentations},
title = {{ACL-IJCNLP 2009 Handbook}},
year = {2009}
}
@article{Xu2008,
abstract = {One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures used in information retrieval such as Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). Several such algorithms including SVMmap and AdaRank have been proposed and their effectiveness has been verified. However, the relationships between the algorithms are not clear, and furthermore no comparisons have been conducted between them. In this paper, we conduct a study on the approach of directly optimizing evaluation measures in learning to rank for Information Retrieval (IR). We focus on the methods that minimize loss functions upper bounding the basic loss function defined on the IR measures. We first provide a general framework for the study and analyze the existing algorithms of SVMmap and AdaRank within the framework. The framework is based on upper bound analysis and two types of upper bounds are discussed. Moreover, we show that we can derive new algorithms on the basis of this analysis and create one example algorithm called PermuRank. We have also conducted comparisons between SVMmap, AdaRank, PermuRank, and conventional methods of Ranking SVM and RankBoost, using benchmark datasets. Experimental results show that the methods based on direct optimization of evaluation measures can always outperform conventional methods of Ranking SVM and RankBoost. However, no significant difference exists among the performances of the direct optimization methods themselves.},
author = {Xu, Jun and Liu, T Y and Lu, Min and Xu, Jun and Lu, Min and Ma, Wei-Ying and Li, Hang and Li, Hang and Ma, W Y and Liu, Tie-Yan},
doi = {10.1145/1390334.1390355},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu et al. - 2008 - Directly optimizing evaluation measures in learning to rank.pdf:pdf},
isbn = {978-1-60558-164-4},
journal = {Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval},
keywords = {all or part of,evaluation measure,information retrieval,is granted without fee,learning to rank,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
number = {49},
pages = {107--114},
title = {{Directly optimizing evaluation measures in learning to rank}},
url = {http://portal.acm.org/citation.cfm?id=1390334.1390355},
year = {2008}
}
@article{Jarvelin2000,
abstract = {This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is de- sirable from the user point of view in modem large IR envi- ronments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on sepa- rate recall bases for documents of different degrees of rele- vance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of rele- vance. The test was run with a best match retrieval system (In- Query I) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differ- ences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous rele- vance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods.},
author = {J{\"{a}}rvelin, Kalervo and Kek{\"{a}}l{\"{a}}inen, Jaana},
doi = {10.1145/345508.345545},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/J{\"{a}}rvelin, Kek{\"{a}}l{\"{a}}inen - 2000 - IR evaluation methods for retrieving highly relevant documents.pdf:pdf},
isbn = {1581132263},
issn = {01635840 (ISSN)},
journal = {Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '00},
pages = {41--48},
title = {{IR evaluation methods for retrieving highly relevant documents}},
url = {http://dl.acm.org/citation.cfm?id=345508.345545},
year = {2000}
}
@article{Phophalia2011,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phophalia - 2011 - A survey on Learning to Rank (LETOR) approaches in information retrieval(2).pdf:pdf},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
pages = {8--10},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Ding,
author = {Ding, Ruoyao},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding - Unknown - Literature survey for Learning to rank.pdf:pdf},
title = {{Literature survey for Learning to rank}}
}
@article{Sahni2007,
author = {Sahni, Saurabh},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sahni - 2007 - Information Retrieval in Resource Constrained Environments.pdf:pdf},
journal = {Information Retrieval},
title = {{Information Retrieval in Resource Constrained Environments}},
year = {2007}
}
@misc{Robertson1976,
abstract = {Abstract 10.1002/asi.4630270302.abs This paper examines statistical techniques for exploiting relevance information to weight search terms. These techniques are presented as a natural extension of weighting methods using information about the distribution of index terms in documents in general. A series of relevance weighting functions is derived and is justified by theoretical considerations. In particular, it is shown that specific weighted search methods are implied by a general probabilistic theory of retrieval. Different applications of relevance weighting are illustrated by experimental results for test collections.},
author = {Robertson, SE and Jones, KS},
booktitle = {Journal of American Society of Information Science},
doi = {10.1002/asi.4630270302},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Robertson, Jones - 1976 - Relevance weighting of search terms.pdf:pdf},
isbn = {0-947568-21-2},
issn = {1097-4571},
number = {3},
pages = {129--146},
pmid = {3048012021851954321},
title = {{Relevance weighting of search terms}},
url = {http://www.marilia.unesp.br/Home/Instituicao/Docentes/EdbertoFerneda/MRI 01 - Robertson SE; Jones KS - 1976.pdf{\%}5Cnhttp://dx.doi.org/10.1002/asi.4630270302{\%}5Cnhttp://onlinelibrary.wiley.com/doi/10.1002/asi.4630270302/abstract?systemMessage=Wiley+Online+Li},
volume = {27},
year = {1976}
}
@article{Ponte2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Ponte, Jay M. and Croft, W. Bruce},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ponte, Croft - 2014 - A Language Modeling Approach to Information Retrieval.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {Igarss 2014},
keywords = {Bott},
number = {1},
pages = {1--5},
pmid = {15003161},
title = {{A Language Modeling Approach to Information Retrieval}},
year = {2014}
}
@article{He2008,
abstract = {Ranking is the key problem for information retrieval and other text applications. Recently, the ranking methods based on machine learning approaches, called learning to rank, become the focus for researchers and practitioners. The main idea of these methods is to apply the various existing and effective algorithms on machine learning to ranking. However, as a learning problem, ranking is different from other classical ones such as classification and regression. In this paper, we investigate the important papers in this direction; the cons and pros of the recent-proposed framework and algorithms for ranking are analyzed, and the relationships among them are discussed. Finally, the promising directions in practice are also pointed out.},
author = {He, Chuan and Wanq, Cong and Zhonq, Yi Xin and Li, Rui Fan},
doi = {10.1109/ICMLC.2008.4620685},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - 2008 - A survey on learning to rank.pdf:pdf},
isbn = {9781424420964},
journal = {Proceedings of the 7th International Conference on Machine Learning and Cybernetics, ICMLC},
keywords = {Evaluation,Information retrieval support vector machine,Learning to rank,Ordinal regression,Ranking},
number = {July},
pages = {1734--1739},
title = {{A survey on learning to rank}},
volume = {3},
year = {2008}
}
@article{Phophalia2011,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phophalia - 2011 - A survey on Learning to Rank (LETOR) approaches in information retrieval(2).pdf:pdf},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
pages = {8--10},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Phophalia2011a,
abstract = {In Recent years, the application of machine learning approaches to conventional IR system evolve a new dimension in the field. The emphasis is now shifted from simply retrieving a set of documents to rank them also for a given query in terms of user's need. The researcher's task is not only to retrieve the documents from the corpus but also to rank them in order of their relevance to the user's requirement. To improve the system's performance is now the hot area of research. In this paper, an attempt has been made to put some of most commonly used algorithms in the community. It presents a survey on the approaches used to rank the retrieved documents and their evaluation strategies.},
author = {Phophalia, Ashish},
doi = {10.1109/NUiConE.2011.6153228},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phophalia - 2011 - A survey on Learning to Rank (LETOR) approaches in information retrieval.pdf:pdf},
isbn = {9781457721694},
journal = {2011 Nirma University International Conference on Engineering: Current Trends in Technology, NUiCONE 2011 - Conference Proceedings},
keywords = {Information Retrieval,Learning to Rank (LETOR),Machine Learning},
number = {December 2011},
title = {{A survey on Learning to Rank (LETOR) approaches in information retrieval}},
year = {2011}
}
@article{Heumann2011,
author = {Heumann, Benjamin W. and Walsh, Stephen J. and McDaniel, Phillip M.},
doi = {10.1016/j.ecoinf.2011.04.004.Assessing},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heumann, Walsh, McDaniel - 2011 - Assessing the Aplicatinon of a Geographic Presence-Only Model for Land Suitability Mapping.pdf:pdf},
journal = {Ecological Informatics},
keywords = {2011 elsevier b,agriculture,all rights reserved,land suitability,maxent,presence-only,thailand,v},
number = {5},
pages = {257--269},
title = {{Assessing the Aplicatinon of a Geographic Presence-Only Model for Land Suitability Mapping}},
volume = {6},
year = {2011}
}
@article{Hijmans2013,
abstract = {This document provides an introduction to species distribution modeling with R . Species distribution modeling (SDM) is also known under other names including climate envelope-modeling, habitat modeling, and (environmental or ecological) niche-modeling. The assumption of SDM is that you can predict the entire, or potential, spatial distribution of a phenomenon, by relating sites of known occurence (and perhaps non-occurrence) with predictor variables known for these sites and for all other sites. The common application of this method is to predict species ranges with climate data as predictors},
archivePrefix = {arXiv},
arxivId = {hep-th/0201144v2},
author = {Hijmans, Robert J and Elith, Jane},
doi = {10.1016/S0550-3213(02)00216-X},
eprint = {0201144v2},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hijmans, Elith - 2013 - Species distribution modeling with R Introduction.pdf:pdf},
isbn = {1574-9541},
issn = {05503213},
journal = {October},
pages = {71},
pmid = {25270536},
primaryClass = {hep-th},
title = {{Species distribution modeling with R Introduction}},
url = {ftp://cran.r-project.org/pub/R/web/packages/dismo/vignettes/sdm.pdf},
year = {2013}
}
@article{Choi1995,
author = {Choi, S P M and Yeung, D.-Y.},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi, Yeung - 1995 - Predictive Q-Routing A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control.pdf:pdf},
journal = {Advances in Neural Information Processing Systems 8},
title = {{Predictive Q-Routing: A Memory-based Reinforcement Learning Approach to Adaptive Traffic Control}},
year = {1995}
}
@article{Safran1995,
abstract = {Clinical data repositories represent a potential gold mine of information and knowledge. Rapid access to such information can help bridge the gap between clinical care and research, support clinical and executive decision making, and improve the quality of care. A clinical database can be used in four ways: to display information about an individual patient (results reporting); to find data on a patient with similarities to one being seen (case finding); to describe a group of patients with at least one attribute in common (cohort description); and to analyze data patterns in terms of trends or relationships (predictive modeling). It seems unlikely that many important clinical questions will be subject to randomized clinical trials because of the ethics, logistics, and expense that would be involved. Evolving statistical and epidemiological methods allow us to approach these clinical data repositories with the purpose of building predictive models, but a clear understanding of the limitations of routinely collected clinical data and the inherent biases is necessary. The largest barrier to using routinely collected clinical data is not the limitations of the data themselves, but rather the lack of a data paradigm for the decision-maker. We present some of the problems and pitfalls in obtaining and using routinely collected data, based upon the use of ClinQuery at Boston's Beth Israel Hospital and the resources and traditions at the Mayo Clinic. ?? 1995.},
archivePrefix = {arXiv},
arxivId = {z0009},
author = {Safran, Charles and Chute, Christopher G.},
doi = {10.1016/0020-7101(94)01094-H},
eprint = {z0009},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Safran, Chute - 1995 - Exploration and exploitation of clinical databases.pdf:pdf},
isbn = {0020-7101 (Print)$\backslash$r0020-7101 (Linking)},
issn = {00207101},
journal = {International Journal of Bio-Medical Computing},
keywords = {Clinical data repositories,Clinical information systems,Clinical research,Computer-based patient records,Exploratory data analysis},
number = {1},
pages = {151--156},
pmid = {7601529},
title = {{Exploration and exploitation of clinical databases}},
volume = {39},
year = {1995}
}
@article{He2017,
author = {He, Author Zi-lin and Wong, Poh-kam and He, Zi-lin},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/He, Wong, He - 2017 - Exploration vs . Exploitation An Empirical Test of the Ambidexterity Hypothesis.pdf:pdf},
keywords = {ambidextrous organization,innovation strategy,technological innovation},
number = {4},
pages = {481--494},
title = {{Exploration vs . Exploitation : An Empirical Test of the Ambidexterity Hypothesis}},
volume = {15},
year = {2017}
}
@article{Kane2017,
author = {Kane, Gerald C and Alavi, Maryam and Science, Source Organization and Technology, Information and Kane, Gerald C},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kane et al. - 2017 - Exploitation Processes Linked references are available on JSTOR for this article Organization Science infnunM Info.pdf:pdf},
keywords = {electronic communities of practice,exploitation,exploration,grant 1996,groupware,knowledge,knowledge management,knowledge portals,many organizations allocate dedicated,organizational learning,repositories,simulation,the ability of organizations,to learn and acquire},
number = {5},
pages = {796--812},
title = {{Exploitation Processes Linked references are available on JSTOR for this article : Organization Science infnunM Information Technology and Organizational Learning : An Investigation of Exploration and Exploitation Processes}},
volume = {18},
year = {2017}
}
@article{Elith2011,
author = {Elith, Jane and Phillips, Steven J and Hastie, Trevor and Dudı, Miroslav},
doi = {10.1111/j.1472-4642.2010.00725.x},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Elith et al. - 2011 - A statistical explanation of MaxEnt for.pdf:pdf},
keywords = {absence,ecological niche,entropy,machine learning,presence-only,species},
pages = {43--57},
title = {{A statistical explanation of MaxEnt for}},
year = {2011}
}
@article{Gupta2017,
author = {Gupta, Anil K and Smith, Ken G and Shalley, Christina E and Smith, K E N G and Shalley, Christina E},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gupta et al. - 2017 - The Interplay between Exploration and Exploitation Linked references are available on JSTOR for this article THE.pdf:pdf},
number = {4},
pages = {693--706},
title = {{The Interplay between Exploration and Exploitation Linked references are available on JSTOR for this article : THE INTERPLAY BETWEEN EXPLORATION AND EXPLOITATION}},
volume = {49},
year = {2017}
}
@article{Zhang2001,
abstract = {One of the most important features of current state-of-the-art SAT solvers is the use of conflict based backtracking and learning techniques. In this paper, we generalize various conflict driven learning strategies in terms of different partitioning schemes of the implication graph. We re-examine the learning techniques used in various SAT solvers and propose an array of new learning schemes. Extensive experiments with real world examples show that the best performing new learning scheme has at least a 2X speedup compared with learning schemes employed in state-of-the-art SAT solvers.},
author = {Zhang, Lintao and Madigan, Conor F and Moskewicz, Matthew H and Malik, Sharad},
doi = {10.1109/ICCAD.2001.968634},
isbn = {0-7803-7249-2},
issn = {1092-3152},
journal = {Proceedings of the 2001 IEEE/ACM International Conference on Computer-aided Design},
keywords = {CDCL,Clause Learning,Learning,SAT,UIP First UIP},
pages = {279--285},
title = {{Efficient Conflict Driven Learning in a Boolean Satisfiability Solver}},
url = {http://dl.acm.org/citation.cfm?id=603095.603153},
year = {2001}
}
@article{Phillips2004,
author = {Phillips, Steven J and Avenue, Park and Park, Florham},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Avenue, Park - 2004 - A Maximum Entropy Approach to Species Distribution Modeling.pdf:pdf},
pages = {655--662},
title = {{A Maximum Entropy Approach to Species Distribution Modeling}},
year = {2004}
}
@article{Phillips2006,
author = {Phillips, Steven J and Anderson, Robert P and Schapire, Robert E},
doi = {10.1016/j.ecolmodel.2005.03.026},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Phillips, Anderson, Schapire - 2006 - Maximum entropy modeling of species geographic distributions.pdf:pdf},
keywords = {distribution,maximum entropy,modeling,niche,range},
pages = {231--259},
title = {{Maximum entropy modeling of species geographic distributions}},
volume = {190},
year = {2006}
}
@article{Trust2017,
author = {Trust, Biometrika},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trust - 2017 - Biometrika Trust A Dynamic Allocation Index for the Discounted Multiarmed Bandit Problem Author ( s ) J . C . Gittins and.pdf:pdf},
number = {3},
pages = {561--565},
title = {{Biometrika Trust A Dynamic Allocation Index for the Discounted Multiarmed Bandit Problem Author ( s ): J . C . Gittins and D . M . Jones Published by : Oxford University Press on behalf of Biometrika Trust Stable URL : http://www.jstor.org/stable/2335176 }},
volume = {66},
year = {2017}
}
@article{,
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2017 - Exploration and Exploitation in Organizational Learning Author ( s ) James G . March Source Organization Science , Vol.pdf:pdf},
number = {1},
pages = {71--87},
title = {{Exploration and Exploitation in Organizational Learning Author ( s ): James G . March Source : Organization Science , Vol . 2 , No . 1 , Special Issue : Organizational Learning : Papers in Honor of ( and by ) James G . March ( 1991 ), pp . 71-87 Published}},
volume = {2},
year = {2017}
}
@article{Gilsing2006,
author = {Gilsing, Victor and Nooteboom, Bart},
doi = {10.1016/j.respol.2005.06.007},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gilsing, Nooteboom - 2006 - Exploration and exploitation in innovation systems The case of pharmaceutical biotechnology.pdf:pdf},
keywords = {exploitation,exploration,pharmaceutical biotechnology,sectoral systems},
pages = {1--23},
title = {{Exploration and exploitation in innovation systems : The case of pharmaceutical biotechnology}},
volume = {35},
year = {2006}
}
@article{Rothaermel2017,
author = {Rothaermel, Frank T and Deeds, David L and Strategic, Source and Journal, Management and Mar, No},
doi = {10.1002/smj.376},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rothaermel et al. - 2017 - Development BIOTECHNOLOGY A SYSTEM OF NEW PRODUCT.pdf:pdf},
number = {3},
pages = {201--221},
title = {{Development BIOTECHNOLOGY : A SYSTEM OF NEW PRODUCT}},
volume = {25},
year = {2017}
}
@article{Pelikan2012,
abstract = {Estimation of distribution algorithms (EDAs) guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. However, EDAs are not only optimization techniques; besides the optimum or its approximation, EDAs provide practitioners with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-specific neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem. This chapter provides an introduction to EDAs as well as a number of pointers for obtaining more information about this class of algorithms.},
author = {Pelikan, Martin and Hauschild, Mark W. and Lobo, Fernando G.},
keywords = {Estimation of distribution algorithms,evolutionary computation,graphical models,stochastic optimization},
number = {2012003},
pages = {42},
title = {{Introduction to Estimation of Distribution Algorithms}},
year = {2012}
}
@article{Pipatsrisawat2010,
author = {Pipatsrisawat, Knot and Darwiche, Adnan},
doi = {10.1007/s10817-009-9156-3},
issn = {15730670},
journal = {Journal of Automated Reasoning},
keywords = {Clause learning,Phase selection heuristic,Satisfiability,Satisfiability solver},
number = {3},
pages = {277--301},
title = {{On modern clause-learning satisfiability solvers}},
volume = {44},
year = {2010}
}
@article{Madera2006,
abstract = {Training Artificial Neural Networks (ANNs) is a very complex task with a high practical relevance in the field of supervised learning. In this chapter, the problem of training ANNs is faced with several Estimation of Distribution Algorithms (EDAs) with different features, exploring both continuous and discrete search spaces. These EDAs have been tested on a benchmark taken from the medicine field. The results have been carefully analyzed, and compared versus those of other algorithms in the literature for the considered problem. Our conclusions are both that our EDAs are competitive with the other compared algorithms, and also that the use of continuous EDAs is advantageous, in general, versus discretizing the search space for the studied problems.},
author = {Madera, Julio and Dorronsoro, Bernab{\'{e}}},
doi = {10.1007/0-387-33416-5_5},
isbn = {9781461515395},
journal = {Metaheuristic Procedures for Training Neutral Networks},
number = {December},
pages = {87--108},
title = {{Estimation of Distribution Algorithms}},
year = {2006}
}
@article{Cai2013,
author = {Cai, Shaowei},
keywords = {algorithm implementation,local search,walksat},
title = {{Faster Implementation for WalkSAT}},
year = {2013}
}
@article{Pipatsrisawat2011,
abstract = {In this work, we improve on existing results on the relationship between proof systems obtained from conflict-driven clause-learning SAT solvers and general resolution. Previous contributions such as those by Beame et al. (2004), Hertel et al. (2008), and Buss et al. (2008) demonstrated that variations on conflict-driven clause-learning SAT solvers corresponded to proof systems as powerful as general resolution. However, the models used in these studies required either an extra degree of non-determinism or a preprocessing step that is not utilized by state-of-the-art SAT solvers in practice. In this paper, we prove that conflict-driven clause-learning SAT solvers yield proof systems that indeed p-simulate general resolution without the need for any additional techniques. Moreover, we show that our result can be generalized to certain other practical variations of the solvers, which are based on different learning schemes and restart policies. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Pipatsrisawat, Knot and Darwiche, Adnan},
doi = {10.1016/j.artint.2010.10.002},
isbn = {3642042430},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Boolean satisfiability,Clause-learning SAT solvers,DPLL,Proof complexity,Resolution proof},
number = {2},
pages = {512--525},
title = {{On the power of clause-learning SAT solvers as resolution engines}},
volume = {175},
year = {2011}
}
@article{Hcbr,
author = {Hcbr, X D},
title = {{HG1t {\&} t ( p vu w ! xu dy ip  {\&}  x  {\pounds} s  d   v eq HG1G I ¨ {\textcopyright} P dc @ e fX YX a ` wxxzyg {\{} w | kTl nmok qpdr {\textcopyright} sut xlDv ( j Dwxxzy {\}}{\{} w | {\~{}} C  y {\#} wxxzy {\}}{\{} w | D  yg  1  yzx  B w    d ¡ B  ( ¢ {\pounds}¢ g ¤ {\&}  i g s ¥ {\S} ¦¨¤  x {\textcopyright}  H}}}
}
@article{Boese1994,
abstract = {We analyze relationships among local minima for the traveling salesman and graph bisection problems under standard neighborhood structures. Our work reveals surprising correlations that suggest a globally convex, or "big valley" structure in these optimization cost surfaces. In conjunction with combinatorial results that sharpen previous analyses, our analysis directly motivates a new adaptive multi-start paradigm for heuristic global optimization, wherein starting points for greedy descent are adaptively derived from the best previously found local minima. We test a simple instance of this method for the traveling salesman problem and obtain very significant speedups over previous multi-start implementations. ?? 1994.},
author = {Boese, Kenneth D. and Kahng, Andrew B. and Muddu, Sudhakar},
doi = {10.1016/0167-6377(94)90065-5},
issn = {01676377},
journal = {Operations Research Letters},
keywords = {Global optimization,Graph bisection,Heuristic search,Multi-start,Stochastic hill-climbing,Traveling salesman problem},
number = {2},
pages = {101--113},
title = {{A new adaptive multi-start technique for combinatorial global optimizations}},
volume = {16},
year = {1994}
}
@article{Adsit2014,
author = {Adsit, Connor and Bradley, Kevin and Heinrich, Christian},
pages = {1--17},
title = {{WalkSAT : Solving Boolean Satisfiability via Stochastic Search}},
year = {2014}
}
@article{Armananzas2008,
abstract = {Evolutionary search algorithms have become an essential asset in the algorithmic toolbox for solving high-dimensional optimization problems in across a broad range of bioinformatics problems. Genetic algorithms, the most well-known and representative evolutionary search technique, have been the subject of the major part of such applications. Estimation of distribution algorithms (EDAs) offer a novel evolutionary paradigm that constitutes a natural and attractive alternative to genetic algorithms. They make use of a probabilistic model, learnt from the promising solutions, to guide the search process. In this paper, we set out a basic taxonomy of EDA techniques, underlining the nature and complexity of the probabilistic model of each EDA variant. We review a set of innovative works that make use of EDA techniques to solve challenging bioinformatics problems, emphasizing the EDA paradigm's potential for further research in this domain.},
author = {Arma{\~{n}}anzas, Rub{\'{e}}n and Inza, I{\~{n}}aki and Santana, Roberto and Saeys, Yvan and Flores, Jose Luis and Lozano, Jose Antonio and {Van de Peer}, Yves and Blanco, Rosa and Robles, V{\'{i}}ctor and Bielza, Concha and Larra{\~{n}}aga, Pedro},
doi = {10.1186/1756-0381-1-6},
isbn = {1756-0381},
issn = {1756-0381},
journal = {BioData mining},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Data Mining and Knowledge Discovery},
number = {1},
pages = {6},
pmid = {18822112},
title = {{A review of estimation of distribution algorithms in bioinformatics.}},
url = {http://biodatamining.biomedcentral.com/articles/10.1186/1756-0381-1-6},
volume = {1},
year = {2008}
}
@article{Tabassum2014,
author = {Tabassum, Mujahid},
doi = {10.17781/P001091},
issn = {2225-658X},
journal = {International Journal of Digital Information and Wireless Communications},
number = {1},
pages = {124--142},
title = {{a Genetic Algorithm Analysis Towards Optimization Solutions}},
url = {http://sdiwc.net/digital-library/a-genetic-algorithm-analysis-towards-optimization-solutions.html},
volume = {4},
year = {2014}
}
@article{Selman1995,
abstract = {It has recently been shown that local search is surprisingly good at nding satisfying assignments for certain classes of CNF formulas In this paper we demonstrate that the power of local search for satissability testing can be further enhanced by employinga new strategy, called $\backslash$mixed random walk", for escaping from local minima. We present experimental results showing how this strategy allows us to handle formulas that are substantially larger than those that can be solved with basic local search. We also present a detailed comparison of our random walk strategy with simulated annealing. Our results show that mixed random walk is the superior strategy on several classes of computationally diicult problem instances. Finally, we present results demonstrating the eeectiveness of local search with walk for solving circuit synthesis and diagnosis problems.},
author = {Selman, Bart and Kautz, Henry and Cohen, Bram},
journal = {DIMACS Series in Discrete Mathematics and Theoretical Computer Science},
title = {{Local Search Strategies for Satissability Testing}},
volume = {00},
year = {1995}
}
@article{Trust2016,
author = {Trust, Biometrika},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Trust - 2016 - Biometrika Trust On the Regression Analysis of Multivariate Failure Time Data Author ( s ) R . L . Prentice , B . J . Wil.pdf:pdf},
number = {2},
pages = {373--379},
title = {{Biometrika Trust On the Regression Analysis of Multivariate Failure Time Data Author ( s ): R . L . Prentice , B . J . Williams and A . V . Peterson Published by : Oxford University Press on behalf of Biometrika Trust Stable URL : http://www.jstor.org/sta}},
volume = {68},
year = {2016}
}
@article{Bousquet2004,
abstract = {The goal of statistical learning theory is to study, in a statistical framework, the properties of learning algorithms. In particular, most results take the form of so-called error bounds. This tutorial introduces the techniques that are used to obtain such results.},
author = {Bousquet, Olivier},
doi = {10.1007/978-3-540-28650-9_8},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bousquet - 2004 - Introduction to Statistical Learning Theory.pdf:pdf},
isbn = {9783540231226},
issn = {03029743},
journal = {Biological Cybernetics},
number = {1},
pages = {169--207},
title = {{Introduction to Statistical Learning Theory}},
url = {http://www.springerlink.com/index/CGW0K6W5W1W1WR9B.pdf},
volume = {3176},
year = {2004}
}
@article{Lu2010,
author = {Lu, Tyler and P{\'{a}}l, D{\'{a}}vid and P{\'{a}}l, Martin},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, P{\'{a}}l, P{\'{a}}l - 2010 - Contextual multi-armed bandits.pdf:pdf},
issn = {15324435},
journal = {International Conference on Artificial Intelligence and Statistics},
pages = {485--492},
title = {{Contextual multi-armed bandits}},
volume = {9},
year = {2010}
}
@article{Miller2012,
author = {Miller, G. and Weatherwax, M. and Gardinier, T. and Abe, N. and Melville, P. and Pendus, C. and Jensen, D. and Reddy, C. K. and Thomas, V. and Bennett, J. and Anderson, G. and Cooley, B.},
doi = {10.1287/inte.1110.0618},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Miller et al. - 2012 - Tax Collections Optimization for New York State.pdf:pdf},
issn = {0092-2102},
journal = {Interfaces},
keywords = {data analysis,decision support systems,dynamic programming,tax policy},
number = {1},
pages = {74--84},
title = {{Tax Collections Optimization for New York State}},
volume = {42},
year = {2012}
}
@article{Problem2016,
author = {Problem, Two-armed Bandit},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Problem - 2016 - Online planning for Multi-armed Bandit Problems.pdf:pdf},
number = {1},
pages = {2--4},
title = {{Online planning for Multi-armed Bandit Problems}},
year = {2016}
}
@article{Jiang2015,
abstract = {For Markov decision processes with long horizons (i.e., dis-count factors close to one), it is common in practice to use reduced horizons during planning to speed computation. However, perhaps surprisingly, when the model available to the agent is estimated from data, as will be the case in most real-world problems, the policy found using a shorter planning horizon can actually be better than a policy learned with the true horizon. In this paper we provide a precise explanation for this phenomenon based on principles of learn-ing theory. We show formally that the planning horizon is a complexity control parameter for the class of policies to be learned. In particular, it has an intuitive, monotonic rela-tionship with a simple counting measure of complexity, and that a similar relationship can be observed empirically with a more general and data-dependent Rademacher complexity measure. Each complexity measure gives rise to a bound on the planning loss predicting that a planning horizon shorter than the true horizon can reduce overfitting and improve test performance, and we confirm these predictions empirically.},
author = {Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2015 - The Dependence of Effective Planning Horizon on Model Accuracy.pdf:pdf},
isbn = {9781450337700},
issn = {15582914},
journal = {Proceedings of the 14th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
keywords = {discount factor,over-fitting,reinforcement learning},
pages = {1181--1189},
title = {{The Dependence of Effective Planning Horizon on Model Accuracy}},
year = {2015}
}
@article{Cabras2007,
abstract = {Threshold selection is a key aspect in extreme values analysis, especially$\backslash$nwhen the sample size is small. The main idea underpinning this work$\backslash$nis that extreme observations are assumed to be outliers of a specified$\backslash$nparametric model. We propose a threshold selection method based on$\backslash$noutlier detection using a suitable measure of surprise. Copyright$\backslash$n(c) 2006 John Wiley {\&} Sons, Ltd.},
author = {Cabras, S and Morales, J},
doi = {10.1002/asmb},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cabras, Morales - 2007 - Extreme Value Analysis within a Parametric Outlier Detection Framework.pdf:pdf},
isbn = {15241904},
issn = {1524-1904},
journal = {Applied Stochastic Models in Business and Industry},
keywords = {generalized pareto distribution,partial posterior predictive distribution,threshold selection},
number = {January},
pages = {157--164},
pmid = {35395390},
title = {{Extreme Value Analysis within a Parametric Outlier Detection Framework}},
volume = {23},
year = {2007}
}
@incollection{Mahajan2008,
abstract = {Multi-armed bandit (MAB) problems are a class of sequential resource allo- cation problems concerned with allocating one or more resources among several alternative (competing) projects. Such problems are paradigms of a fun- damental conflict between making decisions (allocating resources) that yield high current rewards, versus making decisions that sacrifice current gains with the prospect of better future rewards. The MAB formulation models resource allocation problems arising in several technological and scientific disciplines such as sensor management, manufacturing systems, economics, queueing and communication networks, clinical trials, control theory, search theory, etc. (see 88 and references therein).},
author = {Mahajan, Aditya},
booktitle = {Foundations and Applications of Sensor Management},
doi = {10.1007/978-0-387-49819-5_6},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mahajan - 2008 - Multi-Armed BAndit Problems.pdf:pdf},
isbn = {978-0-387-27892-6},
issn = {10959203},
pages = {121-- 151},
pmid = {16917057},
title = {{Multi-Armed BAndit Problems}},
url = {http://www.springerlink.com/index/RX4L35L04K022G37.pdf},
year = {2008}
}
@article{Kim2015,
abstract = {The multiarmed bandit problem is a popular framework for studying the exploration versus exploitation trade-off. Recent applications include dynamic assortment design, Internet advertising, dynamic pricing, and the control of queues. The standard mathematical formulation for a bandit problem makes the strong assumption that the decision maker has a full characterization of the joint distribution of the rewards, and that “arms” under this distribution are independent. These assumptions are not satisfied in many applications, and the out-of-sample performance of policies that optimize a misspecified model can be poor. Motivated by these concerns, we formulate a robust bandit problem in which a decision maker accounts for distrust in the nominal model by solving a worst-case problem against an adversary (“nature”) who has the ability to alter the underlying reward distribution and does so to minimize the decision maker's expected total profit. Structural properties of the optimal worst-case policy are charac...},
author = {Kim, Michael Jong and Lim, Andrew E.B.},
doi = {10.1287/mnsc.2015.2153},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, Lim - 2015 - Robust Multiarmed Bandit Problems.pdf:pdf},
issn = {0025-1909},
journal = {Management Science},
keywords = {bandit problems,games against nature,model uncertainty,relative entropy,robust control},
number = {April},
pages = {150805104205004},
title = {{Robust Multiarmed Bandit Problems}},
year = {2015}
}
@book{Vanderbei2001,
author = {Vanderbei, Robert J},
edition = {2nd},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanderbei - 2001 - Linear Programming Foundations and Extensions.pdf:pdf},
publisher = {Springer},
title = {{Linear Programming: Foundations and Extensions}},
year = {2001}
}
@book{Puterman2005,
author = {Puterman, Martin L},
file = {:home/reazul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Puterman - 2005 - Markov decision processes Discrete stochastic dynamic programming.pdf:pdf},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Markov decision processes: Discrete stochastic dynamic programming}},
year = {2005}
}
